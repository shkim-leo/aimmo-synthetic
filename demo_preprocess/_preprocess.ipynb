{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Generate Class Image for Dreambooth**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "output_path = \"/data/noah/dataset/dreambooth_AD/class/\"\n",
    "model_id = \"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/rv/\"\n",
    "device_id = \"cuda:2\"\n",
    "prompt = \"a photo of the cars in motion, best quality, extremely detailed, clearness, naturalness, film grain, crystal clear, photo with color, actuality\"\n",
    "negative_prompt = \"cartoon, anime, painting, disfigured, immature, blur, picture, ugly, 3D, render, semi-realistic, drawing, poorly drawn, bad anatomy, wrong anatomy, gray scale, worst quality, low quality, sketch\"\n",
    "num_images = 1000\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, use_safetensors=True).to(device_id)\n",
    "\n",
    "for _ in tqdm(range(num_images)):\n",
    "    image = pipe(prompt=prompt, negative_prompt=negative_prompt, guidance_scale=9.0, num_inference_steps=50).images[0]\n",
    "    image.save(os.path.join(output_path, \"{}.jpg\".format(_)))\n",
    "# image.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Get Image Dataset**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "src = \"/data/noah/dataset/batch_3\"\n",
    "dst = \"/data/noah/dataset/AD_REIMAGINE/test\"\n",
    "max_copy = 1000\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for path, dirs, files in os.walk(src):\n",
    "    for f in files:\n",
    "        if os.path.splitext(f)[-1] in [\".jpg\", \".png\"]:\n",
    "            shutil.copy(os.path.join(path, f), os.path.join(dst, f))\n",
    "\n",
    "        if cnt >= max_copy:\n",
    "            break\n",
    "\n",
    "    if cnt >= max_copy:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InterpolationMode\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/__init__.py:1465\u001b[0m\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m library\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m-> 1465\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations\n\u001b[1;32m   1467\u001b[0m \u001b[38;5;66;03m# Enable CUDA Sanitizer\u001b[39;00m\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTORCH_CUDA_SANITIZER\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_meta_registrations.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decomp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _add_op_to_registry, global_decomposition_table, meta_table\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpOverload\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _elementwise_meta, ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_decomp/__init__.py:169\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decompositions\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# populate the table\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decomp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecompositions\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_refs\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# This list was copied from torch/_inductor/decomposition.py\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# excluding decompositions that results in prim ops\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# Resulting opset of decomposition is core aten ops\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_decomp/decompositions.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, cast, Iterable, List, Optional, Tuple, Union\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mprims\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_prims/__init__.py:33\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     check,\n\u001b[1;32m     19\u001b[0m     Dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     type_to_dtype,\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backwards_not_supported\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_subclasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfake_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FakeTensor, FakeTensorMode\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moverrides\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m handle_torch_function, has_torch_function\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pytree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tree_flatten, tree_map, tree_unflatten\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_subclasses/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_subclasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfake_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     DynamicOutputShapeException,\n\u001b[1;32m      5\u001b[0m     FakeTensor,\n\u001b[1;32m      6\u001b[0m     FakeTensorMode,\n\u001b[1;32m      7\u001b[0m     UnsupportedFakeTensorException,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_subclasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfake_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossRefFakeMode\n\u001b[1;32m     12\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFakeTensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFakeTensorMode\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossRefFakeMode\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mweakref\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReferenceType\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Source\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpOverload\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     elementwise_dtypes,\n\u001b[1;32m     17\u001b[0m     ELEMENTWISE_TYPE_PROMOTION_KIND,\n\u001b[1;32m     18\u001b[0m     is_float_dtype,\n\u001b[1;32m     19\u001b[0m     is_integer_dtype,\n\u001b[1;32m     20\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_guards.py:14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# TODO(voz): Stolen pattern, not sure why this is the case,\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# but mypy complains.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo sympy found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sympy/__init__.py:73\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (to_cnf, to_dnf, to_nnf, And, Or, Not, Xor, Nand, Nor,\n\u001b[1;32m     67\u001b[0m         Implies, Equivalent, ITE, POSform, SOPform, simplify_logic, bool_map,\n\u001b[1;32m     68\u001b[0m         true, false, satisfiable)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massumptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (AppliedPredicate, Predicate, AssumptionsContext,\n\u001b[1;32m     71\u001b[0m         assuming, Q, ask, register_handler, remove_handler, refine)\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolys\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (Poly, PurePoly, poly_from_expr, parallel_poly_from_expr,\n\u001b[1;32m     74\u001b[0m         degree, total_degree, degree_list, LC, LM, LT, pdiv, prem, pquo,\n\u001b[1;32m     75\u001b[0m         pexquo, div, rem, quo, exquo, half_gcdex, gcdex, invert,\n\u001b[1;32m     76\u001b[0m         subresultants, resultant, discriminant, cofactors, gcd_list, gcd,\n\u001b[1;32m     77\u001b[0m         lcm_list, lcm, terms_gcd, trunc, monic, content, primitive, compose,\n\u001b[1;32m     78\u001b[0m         decompose, sturm, gff_list, gff, sqf_norm, sqf_part, sqf_list, sqf,\n\u001b[1;32m     79\u001b[0m         factor_list, factor, intervals, refine_root, count_roots, real_roots,\n\u001b[1;32m     80\u001b[0m         nroots, ground_roots, nth_power_roots_poly, cancel, reduced, groebner,\n\u001b[1;32m     81\u001b[0m         is_zero_dimensional, GroebnerBasis, poly, symmetrize, horner,\n\u001b[1;32m     82\u001b[0m         interpolate, rational_interpolate, viete, together,\n\u001b[1;32m     83\u001b[0m         BasePolynomialError, ExactQuotientFailed, PolynomialDivisionFailed,\n\u001b[1;32m     84\u001b[0m         OperationNotSupported, HeuristicGCDFailed, HomomorphismFailed,\n\u001b[1;32m     85\u001b[0m         IsomorphismFailed, ExtraneousFactors, EvaluationFailed,\n\u001b[1;32m     86\u001b[0m         RefinementFailed, CoercionFailed, NotInvertible, NotReversible,\n\u001b[1;32m     87\u001b[0m         NotAlgebraic, DomainError, PolynomialError, UnificationFailed,\n\u001b[1;32m     88\u001b[0m         GeneratorsError, GeneratorsNeeded, ComputationFailed,\n\u001b[1;32m     89\u001b[0m         UnivariatePolynomialError, MultivariatePolynomialError,\n\u001b[1;32m     90\u001b[0m         PolificationFailed, OptionError, FlagError, minpoly,\n\u001b[1;32m     91\u001b[0m         minimal_polynomial, primitive_element, field_isomorphism,\n\u001b[1;32m     92\u001b[0m         to_number_field, isolate, round_two, prime_decomp, prime_valuation,\n\u001b[1;32m     93\u001b[0m         itermonomials, Monomial, lex, grlex,\n\u001b[1;32m     94\u001b[0m         grevlex, ilex, igrlex, igrevlex, CRootOf, rootof, RootOf,\n\u001b[1;32m     95\u001b[0m         ComplexRootOf, RootSum, roots, Domain, FiniteField, IntegerRing,\n\u001b[1;32m     96\u001b[0m         RationalField, RealField, ComplexField, PythonFiniteField,\n\u001b[1;32m     97\u001b[0m         GMPYFiniteField, PythonIntegerRing, GMPYIntegerRing, PythonRational,\n\u001b[1;32m     98\u001b[0m         GMPYRationalField, AlgebraicField, PolynomialRing, FractionField,\n\u001b[1;32m     99\u001b[0m         ExpressionDomain, FF_python, FF_gmpy, ZZ_python, ZZ_gmpy, QQ_python,\n\u001b[1;32m    100\u001b[0m         QQ_gmpy, GF, FF, ZZ, QQ, ZZ_I, QQ_I, RR, CC, EX, EXRAW,\n\u001b[1;32m    101\u001b[0m         construct_domain, swinnerton_dyer_poly, cyclotomic_poly,\n\u001b[1;32m    102\u001b[0m         symmetric_poly, random_poly, interpolating_poly, jacobi_poly,\n\u001b[1;32m    103\u001b[0m         chebyshevt_poly, chebyshevu_poly, hermite_poly, legendre_poly,\n\u001b[1;32m    104\u001b[0m         laguerre_poly, apart, apart_list, assemble_partfrac_list, Options,\n\u001b[1;32m    105\u001b[0m         ring, xring, vring, sring, field, xfield, vfield, sfield)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseries\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (Order, O, limit, Limit, gruntz, series, approximants,\n\u001b[1;32m    108\u001b[0m         residue, EmptySequence, SeqPer, SeqFormula, sequence, SeqAdd, SeqMul,\n\u001b[1;32m    109\u001b[0m         fourier_series, fps, difference_delta, limit_seq)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (factorial, factorial2, rf, ff, binomial,\n\u001b[1;32m    112\u001b[0m         RisingFactorial, FallingFactorial, subfactorial, carmichael,\n\u001b[1;32m    113\u001b[0m         fibonacci, lucas, motzkin, tribonacci, harmonic, bernoulli, bell, euler,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m         Znm, elliptic_k, elliptic_f, elliptic_e, elliptic_pi, beta, mathieus,\n\u001b[1;32m    133\u001b[0m         mathieuc, mathieusprime, mathieucprime, riemann_xi, betainc, betainc_regularized)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sympy/polys/__init__.py:90\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrationaltools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m together\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolyerrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (BasePolynomialError, ExactQuotientFailed,\n\u001b[1;32m     81\u001b[0m         PolynomialDivisionFailed, OperationNotSupported, HeuristicGCDFailed,\n\u001b[1;32m     82\u001b[0m         HomomorphismFailed, IsomorphismFailed, ExtraneousFactors,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m         MultivariatePolynomialError, PolificationFailed, OptionError,\n\u001b[1;32m     88\u001b[0m         FlagError)\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumberfields\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (minpoly, minimal_polynomial, primitive_element,\n\u001b[1;32m     91\u001b[0m         field_isomorphism, to_number_field, isolate, round_two, prime_decomp,\n\u001b[1;32m     92\u001b[0m         prime_valuation)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmonomials\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m itermonomials, Monomial\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01morderings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lex, grlex, grevlex, ilex, igrlex, igrevlex\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sympy/polys/numberfields/__init__.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mminpoly\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m minpoly, minimal_polynomial\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msubfield\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m field_isomorphism, primitive_element, to_number_field\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m isolate\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m round_two\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprimes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prime_decomp, prime_valuation\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sympy/polys/numberfields/utilities.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolys\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdomains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrationalfield\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QQ\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolys\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdomains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegerring\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ZZ\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolys\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatrices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DMRankError\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolys\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumberfields\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mminpoly\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m minpoly\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprinting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlambdarepr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IntervalPrinter\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sympy/polys/matrices/__init__.py:11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03msympy.polys.matrices package.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdomainmatrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DomainMatrix, DM\n\u001b[1;32m     13\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDomainMatrix\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDM\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     15\u001b[0m ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sympy/polys/matrices/domainmatrix.py:25\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstructor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m construct_domain\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (DMNonSquareMatrixError, DMShapeError,\n\u001b[1;32m     22\u001b[0m                          DMDomainError, DMFormatError, DMBadInputError,\n\u001b[1;32m     23\u001b[0m                          DMNotAField)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mddm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DDM\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SDM\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdomainscalar\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DomainScalar\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1012\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:672\u001b[0m, in \u001b[0;36m_compile_bytecode\u001b[0;34m(data, name, bytecode_path, source_path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from diffusers.blip.models.blip import blip_decoder\n",
    "\n",
    "\n",
    "def load_demo_image(image_path, image_size, device):\n",
    "    raw_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    w, h = raw_image.size\n",
    "    # display(raw_image.resize((w//5,h//5)))\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        ]\n",
    "    )\n",
    "    image = transform(raw_image).unsqueeze(0).to(device)\n",
    "    return image\n",
    "\n",
    "\n",
    "image_size = 512\n",
    "image_dir = \"/data/noah/dataset/disney\"\n",
    "model_path = \"/data/noah/ckpt/pretrain_ckpt/BLIP/model_large_caption.pth\"\n",
    "device = \"cuda:2\"\n",
    "torch.cuda.set_device(torch.device(device))\n",
    "\n",
    "model = blip_decoder(pretrained=model_path, image_size=image_size, vit=\"large\")\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "file_name = \"/data/noah/dataset/disney/metadata.csv\"\n",
    "data = []\n",
    "\n",
    "for img_name in tqdm(os.listdir(image_dir)):\n",
    "    # if os.path.splitext(img_name)[-1] not in [\".jpg\", \".png\"]:\n",
    "    #     continue\n",
    "\n",
    "    image_path = os.path.join(image_dir, img_name)\n",
    "\n",
    "    image = load_demo_image(image_path=image_path, image_size=image_size, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # beam search\n",
    "        # caption = model.generate(image, sample=False, num_beams=3, max_length=40, min_length=5)\n",
    "        # nucleus sampling\n",
    "        caption = model.generate(image, sample=True, top_p=0.9, max_length=20, min_length=5)\n",
    "\n",
    "        data.append({\"file_name\": image_path, \"text\": caption[0]})\n",
    "\n",
    "with open(file_name, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"image_path\", \"text\"])\n",
    "\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in data:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"{file_name} 파일이 생성되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Get Models**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "\n",
    "huggingface_hub.snapshot_download(\n",
    "    repo_id=\"runwayml/stable-diffusion-v1-5\",\n",
    "    repo_type=\"model\",\n",
    "    local_dir=\"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/sd\",\n",
    "    local_dir_use_symlinks=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "path = \"/data/noah/inference/reimagine/input\"\n",
    "out_path = \"/data/noah/inference/reimagine/input_2\"\n",
    "\n",
    "if not os.path.isdir(out_path):\n",
    "    os.mkdir(out_path)\n",
    "\n",
    "for _ in os.listdir(path):\n",
    "    img_path = os.path.join(path, _)\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    img = cv2.resize(img, (768, 768))\n",
    "\n",
    "    cv2.imwrite(os.path.join(out_path, _), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "num = 2500\n",
    "meta_path = \"/data/noah/dataset/AD_/train/metadata.csv\"\n",
    "src = \"/data/noah/dataset/AD_/train/\"\n",
    "dst = \"/data/noah/dataset/AD/train/\"\n",
    "\n",
    "data = []\n",
    "\n",
    "# CSV 파일을 읽기 모드로 엽니다.\n",
    "with open(meta_path, \"r\", newline=\"\") as csvfile:\n",
    "    # CSV 파일을 읽기 위한 reader 객체를 생성합니다.\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "\n",
    "    # 각 행을 반복하면서 데이터를 처리합니다.\n",
    "    for idx, row in enumerate(csv_reader):\n",
    "        # 각 행의 데이터는 리스트 형태로 반환됩니다.\n",
    "        # 여기에서 데이터를 사용하거나 출력합니다.\n",
    "        if not idx:\n",
    "            continue\n",
    "\n",
    "        data.append({\"file_name\": row[0], \"text\": row[1]})\n",
    "\n",
    "        if idx >= num:\n",
    "            break\n",
    "\n",
    "with open(os.path.join(dst, \"metadata.csv\"), mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"file_name\", \"text\"])\n",
    "\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in tqdm(data):\n",
    "        writer.writerow(row)\n",
    "        shutil.copy(os.path.join(src, row[\"file_name\"]), os.path.join(dst, row[\"file_name\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "meta_path = \"/data/noah/inference/data_filtering/data_filtering.csv\"\n",
    "data = []\n",
    "\n",
    "# CSV 파일을 읽기 모드로 엽니다.\n",
    "with open(meta_path, \"r\", newline=\"\") as csvfile:\n",
    "    # CSV 파일을 읽기 위한 reader 객체를 생성합니다.\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "\n",
    "    # 각 행을 반복하면서 데이터를 처리합니다.\n",
    "    for idx, row in enumerate(csv_reader):\n",
    "        data.append({\"file_name\": row[0], \"score\": row[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/data/noah/inference/data_filtering/synthetic\"\n",
    "sorted_data = sorted(data, key=lambda x: x[\"score\"])\n",
    "sorted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "target_len = 20\n",
    "top_data = sorted_data[:target_len]\n",
    "top_images = []\n",
    "\n",
    "for t in top_data:\n",
    "    img_path = os.path.join(base_path, t[\"file_name\"])\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    top_images.append(img)\n",
    "\n",
    "plt.figure(figsize=(50, 35))\n",
    "\n",
    "for i in range(target_len):\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(top_images[i])\n",
    "    plt.title(top_dis[i][\"syn_name\"])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Generate ControlNet Dataset**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:59,  4.24s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m shutil\u001b[38;5;241m.\u001b[39mcopy(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(src, row[\u001b[38;5;241m0\u001b[39m]), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/images\u001b[39m\u001b[38;5;124m\"\u001b[39m, row[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     46\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/images\u001b[39m\u001b[38;5;124m\"\u001b[39m, row[\u001b[38;5;241m0\u001b[39m]))\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m masks \u001b[38;5;241m=\u001b[39m \u001b[43mmask_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m seg_map \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((img\u001b[38;5;241m.\u001b[39mheight,img\u001b[38;5;241m.\u001b[39mwidth,\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m     50\u001b[0m color_map \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/MobileSAM/mobile_sam/automatic_mask_generator.py:163\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator.generate\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03mGenerates masks for the given image.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m         the mask, given in XYWH format.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Generate masks\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m mask_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Filter small disconnected regions and holes in masks\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_mask_region_area \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/workspace/MobileSAM/mobile_sam/automatic_mask_generator.py:206\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._generate_masks\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    204\u001b[0m data \u001b[38;5;241m=\u001b[39m MaskData()\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m crop_box, layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(crop_boxes, layer_idxs):\n\u001b[0;32m--> 206\u001b[0m     crop_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_crop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_box\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     data\u001b[38;5;241m.\u001b[39mcat(crop_data)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# Remove duplicate masks between crops\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/MobileSAM/mobile_sam/automatic_mask_generator.py:245\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._process_crop\u001b[0;34m(self, image, crop_box, crop_layer_idx, orig_size)\u001b[0m\n\u001b[1;32m    243\u001b[0m data \u001b[38;5;241m=\u001b[39m MaskData()\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (points,) \u001b[38;5;129;01min\u001b[39;00m batch_iterator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoints_per_batch, points_for_image):\n\u001b[0;32m--> 245\u001b[0m     batch_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcropped_im_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_box\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m     data\u001b[38;5;241m.\u001b[39mcat(batch_data)\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m batch_data\n",
      "File \u001b[0;32m/workspace/MobileSAM/mobile_sam/automatic_mask_generator.py:318\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._process_batch\u001b[0;34m(self, points, im_size, crop_box, orig_size)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# Compress to RLE\u001b[39;00m\n\u001b[1;32m    317\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m uncrop_masks(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m\"\u001b[39m], crop_box, orig_h, orig_w)\n\u001b[0;32m--> 318\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrles\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmask_to_rle_pytorch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmasks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/workspace/MobileSAM/mobile_sam/utils/amg.py:132\u001b[0m, in \u001b[0;36mmask_to_rle_pytorch\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m    124\u001b[0m cur_idxs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m    125\u001b[0m     [\n\u001b[1;32m    126\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mcur_idxs\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mcur_idxs\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m     ]\n\u001b[1;32m    130\u001b[0m )\n\u001b[1;32m    131\u001b[0m btw_idxs \u001b[38;5;241m=\u001b[39m cur_idxs[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m-\u001b[39m cur_idxs[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 132\u001b[0m counts \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;28;01mif\u001b[39;00m tensor[i, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    133\u001b[0m counts\u001b[38;5;241m.\u001b[39mextend(btw_idxs\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m    134\u001b[0m out\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m: [h, w], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcounts\u001b[39m\u001b[38;5;124m\"\u001b[39m: counts})\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from PIL import Image\n",
    "from mobile_sam import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "from mobile_sam import SamAutomaticMaskGenerator\n",
    "import random\n",
    "\n",
    "device = \"cuda:3\"\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "src = \"/data/noah/dataset/AD_SD/train\"\n",
    "meta_path = \"/data/noah/dataset/AD_SD/train/metadata.csv\"\n",
    "\n",
    "dst = \"/data/noah/dataset/AD_CON\"\n",
    "ann_path = \"/data/noah/dataset/AD_CON/train.jsonl\"\n",
    "\n",
    "model_type = \"vit_t\"\n",
    "ckpt_path = \"/data/noah/ckpt/pretrain_ckpt/M_SAM/mobile_sam.pt\"\n",
    "mobile_sam = sam_model_registry[model_type](checkpoint=ckpt_path)\n",
    "mobile_sam.to(device=device)\n",
    "mobile_sam.eval()\n",
    "mask_generator = SamAutomaticMaskGenerator(mobile_sam)\n",
    "\n",
    "data = []\n",
    "\n",
    "with open(meta_path, \"r\", newline=\"\") as csvfile:\n",
    "    # CSV 파일을 읽기 위한 reader 객체를 생성합니다.\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "\n",
    "    # 각 행을 반복하면서 데이터를 처리합니다.\n",
    "    for idx, row in tqdm(enumerate(csv_reader)):\n",
    "        # 각 행의 데이터는 리스트 형태로 반환됩니다.\n",
    "        # 여기에서 데이터를 사용하거나 출력합니다.\n",
    "        if not idx:\n",
    "            continue\n",
    "\n",
    "        data.append(\n",
    "            {\"text\": row[1], \"image\": \"images/\" + row[0], \"conditioning_image\": \"conditioning_images/\" + row[0]}\n",
    "        )\n",
    "        shutil.copy(os.path.join(src, row[0]), os.path.join(dst + \"/images\", row[0]))\n",
    "        img = Image.open(os.path.join(dst + \"/images\", row[0])).convert(\"RGB\")\n",
    "        masks = mask_generator.generate(np.array(img))\n",
    "\n",
    "        seg_map = np.zeros((img.height, img.width, 3))\n",
    "        color_map = []\n",
    "\n",
    "        for mask in masks:\n",
    "            while True:\n",
    "                color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "\n",
    "                if color not in color_map:\n",
    "                    color_map.append(color)\n",
    "                    break\n",
    "\n",
    "            seg_map[mask[\"segmentation\"], :] = color\n",
    "\n",
    "        seg_map = Image.fromarray(seg_map.astype(\"uint8\"))\n",
    "        seg_map.save(os.path.join(dst + \"/conditioning_images\", row[0]))\n",
    "\n",
    "with open(os.path.join(dst, \"train.jsonl\"), encoding=\"utf-8\", mode=\"w\") as f:\n",
    "    for i in data:\n",
    "        f.write(json.dumps(i) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7e30c482634850a1ca74558ea75388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "res101.pth:   0%|          | 0.00/531M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d26fe68841f48b3b1f3313e7f6bb071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "latest_net_G.pth:   0%|          | 0.00/318M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57it [02:21,  2.48s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 39\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     36\u001b[0m data\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     37\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: row[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m row[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconditioning_image\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconditioning_images/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m row[\u001b[38;5;241m0\u001b[39m]}\n\u001b[1;32m     38\u001b[0m )\n\u001b[0;32m---> 39\u001b[0m \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdst\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/images\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/images\u001b[39m\u001b[38;5;124m\"\u001b[39m, row[\u001b[38;5;241m0\u001b[39m]))\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     41\u001b[0m leres_img \u001b[38;5;241m=\u001b[39m leres(np\u001b[38;5;241m.\u001b[39marray(img))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/shutil.py:417\u001b[0m, in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(dst):\n\u001b[1;32m    416\u001b[0m     dst \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(src))\n\u001b[0;32m--> 417\u001b[0m \u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m copymode(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/shutil.py:267\u001b[0m, in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _USE_CP_SENDFILE:\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m         \u001b[43m_fastcopy_sendfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfsrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfdst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m dst\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m _GiveupOnFastCopy:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/shutil.py:142\u001b[0m, in \u001b[0;36m_fastcopy_sendfile\u001b[0;34m(fsrc, fdst)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m         sent \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msendfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutfd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;66;03m# ...in oder to have a more informative exception.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         err\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;241m=\u001b[39m fsrc\u001b[38;5;241m.\u001b[39mname\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from PIL import Image\n",
    "from controlnet_aux import LeresDetector\n",
    "import random\n",
    "\n",
    "device = \"cuda:3\"\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "src = \"/data/noah/dataset/AD_SD/train\"\n",
    "meta_path = \"/data/noah/dataset/AD_SD/train/metadata.csv\"\n",
    "\n",
    "dst = \"/data/noah/dataset/AD_CON\"\n",
    "ann_path = \"/data/noah/dataset/AD_CON/train.jsonl\"\n",
    "\n",
    "\n",
    "data = []\n",
    "leres = LeresDetector.from_pretrained(\"lllyasviel/Annotators\")\n",
    "\n",
    "with open(meta_path, \"r\", newline=\"\") as csvfile:\n",
    "    # CSV 파일을 읽기 위한 reader 객체를 생성합니다.\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "\n",
    "    # 각 행을 반복하면서 데이터를 처리합니다.\n",
    "    for idx, row in tqdm(enumerate(csv_reader)):\n",
    "        # 각 행의 데이터는 리스트 형태로 반환됩니다.\n",
    "        # 여기에서 데이터를 사용하거나 출력합니다.\n",
    "        if not idx:\n",
    "            continue\n",
    "\n",
    "        data.append(\n",
    "            {\"text\": row[1], \"image\": \"images/\" + row[0], \"conditioning_image\": \"conditioning_images/\" + row[0]}\n",
    "        )\n",
    "        shutil.copy(os.path.join(src, row[0]), os.path.join(dst + \"/images\", row[0]))\n",
    "        img = Image.open(os.path.join(dst + \"/images\", row[0])).convert(\"RGB\")\n",
    "        leres_img = leres(np.array(img))\n",
    "        leres_img.save(os.path.join(dst + \"/conditioning_images\", row[0]))\n",
    "\n",
    "with open(os.path.join(dst, \"train.jsonl\"), encoding=\"utf-8\", mode=\"w\") as f:\n",
    "    for i in data:\n",
    "        f.write(json.dumps(i) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAAD0lEQVR4nGNgGAWjgJYAAAJiAAHFL8rUAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=20x10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = np.zeros((10, 20))\n",
    "a.shape\n",
    "b = Image.fromarray(a).convert(\"RGB\")\n",
    "display(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "path = \"/data/noah/inference/reimagine/input\"\n",
    "out = \"/data/noah/inference/reimagine/output_i\"\n",
    "\n",
    "for _ in os.listdir(path):\n",
    "    img = cv2.imread(os.path.join(path, _))\n",
    "\n",
    "    img = cv2.resize(img, (768, 768))\n",
    "\n",
    "    cv2.imwrite(os.path.join(out, _), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "from controlnet_aux import CannyDetector, SamDetector\n",
    "\n",
    "# load image\n",
    "img = Image.open(\"/data/noah/dataset/AD/train/1652247588842_FR-View-CMR-Wide.png\").convert(\"RGB\").resize((512, 512))\n",
    "\n",
    "sam = SamDetector.from_pretrained(\"ybelkada/segment-anything\", subfolder=\"checkpoints\")\n",
    "\n",
    "processed_image_2 = sam(img)\n",
    "\n",
    "display(img)\n",
    "display(processed_image_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preprocess Sequence Datasset**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda:3\"\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"/data/noah/ckpt/pretrain_ckpt/GIT\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/data/noah/ckpt/pretrain_ckpt/GIT\").to(device)\n",
    "\n",
    "# set seed for reproducability\n",
    "\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    \"\"\"\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    \"\"\"\n",
    "    Sample a given number of frame indices from the video.\n",
    "    Args:\n",
    "        clip_len (`int`): Total number of frames to sample.\n",
    "        frame_sample_rate (`int`): Sample every n-th frame.\n",
    "        seg_len (`int`): Maximum allowed index of sample's last frame.\n",
    "    Returns:\n",
    "        indices (`List[int]`): List of sampled frame indices\n",
    "    \"\"\"\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices\n",
    "\n",
    "\n",
    "# load video\n",
    "file_path = \"/data/noah/inference/incabin_sample/reference_vid/sample_2.mp4\"\n",
    "container = av.open(file_path)\n",
    "\n",
    "# sample frames\n",
    "num_frames = model.config.num_image_with_embedding\n",
    "indices = sample_frame_indices(clip_len=num_frames, frame_sample_rate=4, seg_len=container.streams.video[0].frames)\n",
    "frames = read_video_pyav(container, indices)\n",
    "\n",
    "pixel_values = processor(images=list(frames), return_tensors=\"pt\").pixel_values\n",
    "generated_ids = model.generate(pixel_values=pixel_values.to(device), max_length=50)\n",
    "\n",
    "print(\"Generated caption:\", processor.batch_decode(generated_ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dataset_base_path = \"/data/noah/dataset/AD_SEQ\"\n",
    "folder_paths = []\n",
    "\n",
    "for root, dirs, files in os.walk(dataset_base_path):\n",
    "    for file in files:\n",
    "        if root not in folder_paths and file[-4:] in [\".jpg\", \".png\", \".jpeg\"]:\n",
    "            folder_paths.append(root)\n",
    "\n",
    "folder_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda:2\"\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"/data/noah/ckpt/pretrain_ckpt/GIT\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/data/noah/ckpt/pretrain_ckpt/GIT\").to(device)\n",
    "\n",
    "# set seed for reproducability\n",
    "\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    \"\"\"\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    \"\"\"\n",
    "    Sample a given number of frame indices from the video.\n",
    "    Args:\n",
    "        clip_len (`int`): Total number of frames to sample.\n",
    "        frame_sample_rate (`int`): Sample every n-th frame.\n",
    "        seg_len (`int`): Maximum allowed index of sample's last frame.\n",
    "    Returns:\n",
    "        indices (`List[int]`): List of sampled frame indices\n",
    "    \"\"\"\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices\n",
    "\n",
    "\n",
    "# load video\n",
    "folder_path = \"/data/noah/dataset/AD_SEQ\"\n",
    "captions = []\n",
    "folder_paths = []\n",
    "\n",
    "for root, folders, files in os.walk(folder_path):\n",
    "    frames = []\n",
    "    for file in files:\n",
    "        if file.endswith(\".png\"):\n",
    "            if root not in folder_paths:\n",
    "                folder_paths.append(root)\n",
    "            frame = Image.open(os.path.join(root, file)).convert(\"RGB\")\n",
    "            frames.append(frame)\n",
    "\n",
    "    ###\n",
    "    if not len(frames):\n",
    "        continue\n",
    "    seg_len = len(frames)\n",
    "    num_frames = model.config.num_image_with_embedding\n",
    "    indices = sorted(\n",
    "        sample_frame_indices(clip_len=num_frames, frame_sample_rate=seg_len // num_frames, seg_len=seg_len)\n",
    "    )\n",
    "    selected_frames = [frames[idx] for idx in indices]\n",
    "    pixel_values = processor(images=selected_frames, return_tensors=\"pt\").pixel_values\n",
    "    generated_ids = model.generate(pixel_values=pixel_values.to(device), max_length=50)\n",
    "    captions.append(processor.batch_decode(generated_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "out_path = \"/data/noah/dataset/AD_SEQ/metadata.csv\"\n",
    "\n",
    "with open(out_path, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"folder\", \"caption\"])\n",
    "\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in zip(folder_paths):\n",
    "        writer.writerow({\"folder\": row})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Videos from Source Video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from diffusers.utils import export_to_video\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "videos_path = \"/data/noah/inference/simulation/video_src\"\n",
    "out_video_path = \"/data/noah/inference/simulation/video\"\n",
    "\n",
    "\n",
    "def extract_video(video_path, extact_frame=16, sample_step=8):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    video_name = os.path.basename(video_path)\n",
    "    frames = []\n",
    "    cnt = 0\n",
    "    sample_cnt = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        sample_cnt += 1\n",
    "\n",
    "        if sample_cnt % 4 != 0:\n",
    "            continue\n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = PILImage.fromarray(frame)\n",
    "        frames.append(frame)\n",
    "\n",
    "        if len(frames) == 16:\n",
    "            out_path = os.path.join(out_video_path, \"{}_{}.mp4\".format(video_name[:-4], cnt))\n",
    "            export_to_video(frames, out_path, fps=5)\n",
    "            cnt += 1\n",
    "            frames = []\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "\n",
    "for video_name in os.listdir(videos_path):\n",
    "    video_path = os.path.join(videos_path, video_name)\n",
    "    extract_video(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quration for Sequence Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
