{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert coco to aimmo\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "parent_path = \"/data/noah/dataset/coco/pre_images\"\n",
    "out_annotation_path = \"/data/noah/dataset/coco/pre_anno\"\n",
    "annotation_path = \"/data/noah/dataset/coco/instances_train2017.json\"\n",
    "\n",
    "with open(annotation_path, \"r\") as f:\n",
    "    annotation = json.load(f)\n",
    "\n",
    "# print(annotation)\n",
    "\n",
    "target_class = [\"person\"]  # bicycle #motorcycle\n",
    "target_ids = []\n",
    "for target in target_class:\n",
    "    for cat in annotation[\"categories\"]:\n",
    "        if cat[\"name\"] == target:\n",
    "            target_ids.append(cat[\"id\"])\n",
    "\n",
    "assert len(target_class) == len(target_ids), \"target_class must have same length with target_ids\"\n",
    "\n",
    "annotation_infos = annotation[\"annotations\"]\n",
    "\n",
    "for image_info in tqdm(annotation[\"images\"]):\n",
    "    image_name = image_info[\"file_name\"]\n",
    "    height, width = image_info[\"height\"], image_info[\"width\"]\n",
    "    image_id = image_info[\"id\"]\n",
    "\n",
    "    aimmo_format = {\n",
    "        \"annotations\": [],\n",
    "        \"attributes\": {},\n",
    "        \"filename\": image_name,\n",
    "        \"parent_path\": parent_path,\n",
    "        \"metadata\": {\"height\": height, \"width\": width},\n",
    "    }\n",
    "\n",
    "    annotations = []\n",
    "    for idx, ann in enumerate(annotation_infos):\n",
    "        if ann[\"image_id\"] == image_id:\n",
    "            for idx, target_id in enumerate(target_ids):\n",
    "                if ann[\"category_id\"] == target_id:\n",
    "                    points = ann[\"segmentation\"]\n",
    "\n",
    "                    if isinstance(points, dict):\n",
    "                        continue\n",
    "                    else:\n",
    "                        polygon_points = np.array(points[0]).reshape(-1, 2).astype(np.int32).tolist()\n",
    "\n",
    "                    ann = {\n",
    "                        \"id\": \"\",\n",
    "                        \"type\": \"poly_seg\",\n",
    "                        \"attributes\": {},\n",
    "                        \"points\": polygon_points,\n",
    "                        \"label\": target_class[idx],\n",
    "                    }\n",
    "                    annotations.append(ann)\n",
    "\n",
    "    if len(annotations):\n",
    "        aimmo_format[\"annotations\"] = annotations\n",
    "        path = os.path.join(out_annotation_path, \"{}.json\".format(image_name))\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(aimmo_format, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# import matplotlib.pyplot as plt\n",
    "# import io\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# path = \"/data/noah/dataset/coco/data\"\n",
    "# image_path = \"/data/noah/dataset/coco/images\"\n",
    "# text_path = \"/data/noah/dataset/coco/text\"\n",
    "\n",
    "# for _ in tqdm(os.listdir(path)):\n",
    "#     n_path = os.path.join(path, _)\n",
    "\n",
    "#     data = pd.read_parquet(n_path)\n",
    "\n",
    "#     # print(data.keys())\n",
    "#     # print(data[\"image\"][0].keys())\n",
    "\n",
    "#     # image = Image.open(io.BytesIO(data[\"image\"][0]['bytes']))\n",
    "#     # display(image)\n",
    "\n",
    "#     for index, row in data.iterrows():\n",
    "#         # print(row[\"sentences_raw\"])\n",
    "#         filename = row[\"filename\"]\n",
    "#         image_bytes = row[\"image\"][\"bytes\"]\n",
    "#         image = Image.open(io.BytesIO(image_bytes))\n",
    "#         image.save(os.path.join(image_path, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, sys\n",
    "\n",
    "# sys.path.append(os.path.join(os.getcwd(), \"GroundingDINO\"))\n",
    "\n",
    "# from IPython.display import display\n",
    "# from PIL import Image, ImageDraw, ImageFont\n",
    "# from torchvision.ops import box_convert\n",
    "\n",
    "# # Grounding DINO\n",
    "# import groundingdino.datasets.transforms as T\n",
    "# from groundingdino.models import build_model\n",
    "# from groundingdino.util import box_ops\n",
    "# from groundingdino.util.slconfig import SLConfig\n",
    "# from groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "# from groundingdino.util.inference import annotate, load_image, predict\n",
    "\n",
    "# import supervision as sv\n",
    "\n",
    "# # segment anything\n",
    "# from segment_anything import build_sam, SamPredictor\n",
    "\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import PIL\n",
    "# import torch\n",
    "# import json\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "# def load_model(model_config_path, model_checkpoint_path, device):\n",
    "#     args = SLConfig.fromfile(model_config_path)\n",
    "#     args.device = device\n",
    "#     model = build_model(args)\n",
    "#     checkpoint = torch.load(model_checkpoint_path, map_location=\"cpu\")\n",
    "#     load_res = model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n",
    "#     print(load_res)\n",
    "#     _ = model.eval()\n",
    "#     return model\n",
    "\n",
    "\n",
    "# # detect object using grounding DINO\n",
    "# def detect(image, image_source, text_prompt, model, box_threshold=0.75, text_threshold=0.5):\n",
    "#     boxes, logits, phrases = predict(\n",
    "#         model=model, image=image, caption=text_prompt, box_threshold=box_threshold, text_threshold=text_threshold\n",
    "#     )\n",
    "\n",
    "#     annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "#     annotated_frame = annotated_frame[..., ::-1]  # BGR to RGB\n",
    "#     return annotated_frame, boxes\n",
    "\n",
    "\n",
    "# def segment(image, sam_model, boxes):\n",
    "#     sam_model.set_image(image)\n",
    "#     H, W, _ = image.shape\n",
    "#     boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
    "\n",
    "#     transformed_boxes = sam_model.transform.apply_boxes_torch(boxes_xyxy.to(device), image.shape[:2])\n",
    "#     masks, _, _ = sam_model.predict_torch(\n",
    "#         point_coords=None,\n",
    "#         point_labels=None,\n",
    "#         boxes=transformed_boxes,\n",
    "#         multimask_output=False,\n",
    "#     )\n",
    "#     return masks.cpu()\n",
    "\n",
    "\n",
    "# def draw_mask(mask, image, random_color=True):\n",
    "#     if random_color:\n",
    "#         color = np.concatenate([np.random.random(3), np.array([0.8])], axis=0)\n",
    "#     else:\n",
    "#         color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\n",
    "#     h, w = mask.shape[-2:]\n",
    "#     mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "\n",
    "#     annotated_frame_pil = Image.fromarray(image).convert(\"RGBA\")\n",
    "#     mask_image_pil = Image.fromarray((mask_image.cpu().numpy() * 255).astype(np.uint8)).convert(\"RGBA\")\n",
    "\n",
    "#     return np.array(Image.alpha_composite(annotated_frame_pil, mask_image_pil))\n",
    "\n",
    "\n",
    "# # def mask_to_polygon(mask):\n",
    "# #     # 윤곽선 찾기\n",
    "# #     contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# #     # 윤곽선을 다각형으로 변환\n",
    "# #     print(contours)\n",
    "# #     polygons = []\n",
    "# #     for contour in contours:\n",
    "# #         polygon = contour.tolist()\n",
    "# #         print(polygon)\n",
    "# #         polygons.append(polygon)\n",
    "\n",
    "\n",
    "# #     return polygons\n",
    "# def mask_to_polygon(mask):\n",
    "#     # 윤곽선 찾기\n",
    "#     contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "#     # 윤곽선을 다각형으로 변환\n",
    "#     polygons = []\n",
    "#     for contour in contours:\n",
    "#         contour = contour.squeeze(axis=1)  # 차원 축소\n",
    "#         polygon = contour[:, [0, 1]].tolist()  # (y, x) 순서로 변환하여 리스트로 저장\n",
    "#         polygons.append(polygon)\n",
    "\n",
    "#     return polygons\n",
    "\n",
    "\n",
    "# def polygon_to_mask(polygons, height, width):\n",
    "#     # 빈 마스크 생성\n",
    "#     mask = np.zeros((height, width), dtype=np.uint8)\n",
    "#     polygons = np.array(polygons, dtype=np.int32)\n",
    "#     mask = cv2.fillPoly(mask, [polygons], 255)\n",
    "\n",
    "#     return mask\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda:3\")\n",
    "# torch.cuda.set_device(device)\n",
    "\n",
    "# grounding_dino_ckpt_path = \"/data/noah/ckpt/pretrain_ckpt/Grounding_DINO/groundingdino_swinb_cogcoor.pth\"\n",
    "# grounding_dino_config_path = (\n",
    "#     \"/workspace/Grounded-Segment-Anything/GroundingDINO/groundingdino/config/GroundingDINO_SwinB.py\"\n",
    "# )\n",
    "# grounding_dino = load_model(grounding_dino_config_path, grounding_dino_ckpt_path, device=device)\n",
    "\n",
    "# sam_ckpt_path = \"/data/noah/ckpt/pretrain_ckpt/SAM/sam_vit_h_4b8939.pth\"\n",
    "# sam_predictor = SamPredictor(build_sam(checkpoint=sam_ckpt_path).to(device))\n",
    "\n",
    "# image_path = \"/data/noah/dataset/coco/pre_images\"\n",
    "# anno_path = \"/data/noah/dataset/coco/pre_anno\"\n",
    "\n",
    "# for image_name in tqdm(os.listdir(image_path)):\n",
    "#     img_path = os.path.join(image_path, image_name)\n",
    "#     image_source_np, image_torch = load_image(img_path)\n",
    "\n",
    "#     # print(type(image_source))\n",
    "#     # print(type(image))\n",
    "#     height, width = image_source_np.shape[0], image_source_np.shape[1]\n",
    "\n",
    "#     anno_file_path = os.path.join(anno_path, image_name[:-4] + \".json\")\n",
    "#     anno = {\n",
    "#         \"annotations\": [],\n",
    "#         \"attributes\": {},\n",
    "#         \"filename\": image_name,\n",
    "#         \"parent_path\": image_path,\n",
    "#         \"metadata\": {\"height\": height, \"width\": width},\n",
    "#     }\n",
    "\n",
    "#     annotated_frame, detected_boxes = detect(image_torch, image_source_np, text_prompt=\"person\", model=grounding_dino)\n",
    "\n",
    "#     if len(detected_boxes) == 0:\n",
    "#         continue\n",
    "\n",
    "#     seg_result = segment(image_source_np, sam_predictor, boxes=detected_boxes)\n",
    "\n",
    "#     for seg_map in seg_result:\n",
    "#         annotation = {\"id\": \"\", \"type\": \"poly_seg\", \"attributes\": {}, \"points\": [], \"label\": \"person\"}\n",
    "\n",
    "#         mask = seg_map[0].cpu().numpy().astype(np.uint8) * 255\n",
    "#         # display(Image.fromarray(mask))\n",
    "\n",
    "#         polygons = mask_to_polygon(mask)[0]\n",
    "#         # print(polygons)\n",
    "\n",
    "#         # mask = polygon_to_mask(polygons, 1024, 1920)\n",
    "#         # display(Image.fromarray(mask))\n",
    "\n",
    "#         annotation[\"points\"] = polygons\n",
    "#         anno[\"annotations\"].append(annotation)\n",
    "\n",
    "#     with open(anno_file_path, \"w\") as f:\n",
    "#         json.dump(anno, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def estimate_blur(image):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Compute Laplacian variance (a measure of sharpness)\n",
    "    laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "    return laplacian_var\n",
    "\n",
    "\n",
    "anno_path = \"/data/noah/dataset/coco/pre_anno\"\n",
    "image_path = \"/data/noah/dataset/coco/pre_images\"\n",
    "out_anno_path = \"/data/noah/dataset/coco/anno\"\n",
    "out_image_path = \"/data/noah/dataset/coco/images\"\n",
    "\n",
    "sizes = []\n",
    "blur_scores = []\n",
    "class_name = \"person\"\n",
    "\n",
    "for anno_name in tqdm(os.listdir(anno_path)[:]):\n",
    "    path = os.path.join(anno_path, anno_name)\n",
    "    with open(path, \"r\") as f:\n",
    "        anno = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    # image = cv2.imread(os.path.join(image_path, anno[\"parent_path\"][1:], anno[\"filename\"]))\n",
    "    image = cv2.imread(os.path.join(anno[\"parent_path\"], anno[\"filename\"]))\n",
    "    blur_score = estimate_blur(image)\n",
    "    blur_scores.append(blur_score)\n",
    "\n",
    "    for ann in anno[\"annotations\"]:\n",
    "        if ann[\"label\"] == class_name:\n",
    "            mask = np.zeros((anno[\"metadata\"][\"height\"], anno[\"metadata\"][\"width\"]))\n",
    "            points = np.array(ann[\"points\"], dtype=np.int32)\n",
    "\n",
    "            try:\n",
    "                cv2.fillPoly(mask, [points], color=255)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            area = np.count_nonzero(mask == 255)\n",
    "            sizes.append(area)\n",
    "\n",
    "# 리스트의 인덱스를 x 값으로 사용\n",
    "x = list(range(len(sizes)))\n",
    "# sizes = sorted(sizes)\n",
    "\n",
    "# 선 그래프로 시각화\n",
    "plt.hist(sizes, bins=100)\n",
    "# plt.plot(x, sizes)\n",
    "plt.xlabel(\"Area\")\n",
    "plt.ylabel(\"num\")\n",
    "plt.title(\"\")\n",
    "plt.show()\n",
    "\n",
    "min_area = np.mean(sizes)\n",
    "print(\"min_area : {}\".format(min_area))\n",
    "\n",
    "# 리스트의 인덱스를 x 값으로 사용\n",
    "x = list(range(len(blur_scores)))\n",
    "# sizes = sorted(sizes)\n",
    "\n",
    "# 선 그래프로 시각화\n",
    "plt.hist(blur_scores, bins=100)\n",
    "# plt.plot(x, sizes)\n",
    "plt.xlabel(\"blur score\")\n",
    "plt.ylabel(\"num\")\n",
    "plt.title(\"\")\n",
    "plt.show()\n",
    "\n",
    "min_blur = np.mean(blur_scores)\n",
    "print(\"min_blur : {}\".format(min_blur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "min_area = min_area * 0.5\n",
    "min_blur = min_blur * 0.25\n",
    "\n",
    "for anno_name in tqdm(os.listdir(anno_path)[:]):\n",
    "    path = os.path.join(anno_path, anno_name)\n",
    "    with open(path, \"r\") as f:\n",
    "        anno = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    image = cv2.imread(os.path.join(anno[\"parent_path\"], anno[\"filename\"]))\n",
    "    # image = cv2.imread(os.path.join(image_path, anno[\"parent_path\"][1:], anno[\"filename\"]))\n",
    "    blur_score = estimate_blur(image)\n",
    "\n",
    "    if blur_score < min_blur:\n",
    "        continue\n",
    "\n",
    "    new_annos = []\n",
    "\n",
    "    for ann in anno[\"annotations\"]:\n",
    "        if ann[\"label\"] == class_name:\n",
    "            mask = np.zeros((anno[\"metadata\"][\"height\"], anno[\"metadata\"][\"width\"]))\n",
    "            points = np.array(ann[\"points\"], dtype=np.int32)\n",
    "\n",
    "            try:\n",
    "                cv2.fillPoly(mask, [points], color=255)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            area = np.count_nonzero(mask == 255)\n",
    "\n",
    "            if area >= min_area:\n",
    "                new_annos.append(ann)\n",
    "\n",
    "    if len(new_annos):\n",
    "        # src_path = os.path.join(image_path, anno[\"parent_path\"][1:], anno[\"filename\"])\n",
    "        src_path = os.path.join(anno[\"parent_path\"], anno[\"filename\"])\n",
    "        dst_path = os.path.join(out_image_path, anno[\"filename\"])\n",
    "        shutil.copy(src_path, dst_path)\n",
    "\n",
    "        anno[\"annotations\"] = new_annos\n",
    "        anno[\"parent_path\"] = out_image_path\n",
    "\n",
    "        with open(os.path.join(out_anno_path, anno_name), \"w\") as f:\n",
    "            json.dump(anno, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15889/15889 [21:56<00:00, 12.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# Format {\"image\": \"\", \"conditioning_images\": \"\", \"text\": \"\", \"annotation\": \"\"}\n",
    "\n",
    "# annotation -> mask -> condition image 생성\n",
    "# prompt\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from PIL import Image\n",
    "from controlnet_aux.processor import Processor\n",
    "from diffusers.blip.models.blip import blip_decoder\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from controlnet_aux.processor import MidasDetector\n",
    "\n",
    "\n",
    "anno_path = \"/data/noah/dataset/coco/anno\"\n",
    "condition_path = \"/data/noah/dataset/coco/conditioning_images\"\n",
    "midas = MidasDetector.from_pretrained(\"lllyasviel/Annotators\").to(\"cuda:3\")\n",
    "\n",
    "for anno_name in tqdm(os.listdir(anno_path)[22184:]):\n",
    "    ann_path = os.path.join(anno_path, anno_name)\n",
    "    with open(ann_path, \"r\") as f:\n",
    "        ann = json.load(f)\n",
    "\n",
    "    # con_path\n",
    "    image_path = os.path.join(ann[\"parent_path\"], ann[\"filename\"])\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    con_path = os.path.join(condition_path, ann[\"filename\"])\n",
    "    con_image = midas(image)\n",
    "    con_image.save(con_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from PIL import Image\n",
    "from controlnet_aux.processor import Processor\n",
    "from diffusers.blip.models.blip import blip_decoder\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "annotation_path = \"/data/noah/dataset/coco/anno\"\n",
    "caption_path = \"/data/noah/dataset/coco/train.jsonl\"\n",
    "gt_caption_path = \"/data/noah/dataset/coco/src_anno/captions_train2017.json\"\n",
    "\n",
    "data = []\n",
    "\n",
    "with open(gt_caption_path, \"r\") as f:\n",
    "    gt_ann = json.load(f)\n",
    "\n",
    "for annotation_name in tqdm(os.listdir(annotation_path)):\n",
    "    anno_path = os.path.join(annotation_path, annotation_name)\n",
    "    with open(anno_path, \"r\") as f:\n",
    "        annotation = json.load(f)\n",
    "\n",
    "    for img_info in gt_ann[\"images\"]:\n",
    "        if img_info[\"file_name\"] == annotation[\"filename\"]:\n",
    "            image_id = img_info[\"id\"]\n",
    "            break\n",
    "\n",
    "    captions = []\n",
    "    for cap_info in gt_ann[\"annotations\"]:\n",
    "        if cap_info[\"image_id\"] == image_id:\n",
    "            captions.append(cap_info[\"caption\"])\n",
    "\n",
    "    caption = random.choice(captions)\n",
    "\n",
    "    data.append(\n",
    "        {\n",
    "            \"text\": caption,\n",
    "            \"image\": \"images/{}\".format(annotation[\"filename\"]),\n",
    "            \"conditioning_images\": \"conditioning_images/{}\".format(annotation[\"filename\"]),\n",
    "            \"annotation\": \"anno/{}\".format(annotation_name),\n",
    "        }\n",
    "    )\n",
    "\n",
    "with open(caption_path, encoding=\"utf-8\", mode=\"w\") as f:\n",
    "    for i in data:\n",
    "        f.write(json.dumps(i) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
