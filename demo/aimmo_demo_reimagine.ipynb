{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image Variant with Stable Unclip**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from diffusers import StableUnCLIPImg2ImgPipeline\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "from diffusers.blip.models.blip import blip_decoder\n",
    "\n",
    "\n",
    "def load_demo_image(image_path, image_size, device):\n",
    "    raw_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    w, h = raw_image.size\n",
    "    # display(raw_image.resize((w//5,h//5)))\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        ]\n",
    "    )\n",
    "    image = transform(raw_image).unsqueeze(0).to(device)\n",
    "    return image\n",
    "\n",
    "\n",
    "# Start the StableUnCLIP Image variations pipeline\n",
    "device = \"cuda:3\"\n",
    "torch.cuda.set_device(torch.device(device))  # change allocation of current GPU\n",
    "base_model_path = \"/data/noah/ckpt/finetuning/SD_UNCLIP_AD\"\n",
    "\n",
    "clip_image_size = 512\n",
    "clip_model_path = \"/data/noah/ckpt/pretrain_ckpt/BLIP/model_large_caption.pth\"\n",
    "clip_model = blip_decoder(pretrained=clip_model_path, image_size=clip_image_size, vit=\"large\")\n",
    "clip_model.eval()\n",
    "clip_model = clip_model.to(device)\n",
    "\n",
    "pipe = StableUnCLIPImg2ImgPipeline.from_pretrained(base_model_path, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "path = \"/data/noah/inference/reimagine_exam/1657066276420_FR-View-CMR-Wide.png\"\n",
    "\n",
    "# Pipe to make the variation\n",
    "init_image = Image.open(path).convert(\"RGB\")\n",
    "init_image.thumbnail((768, 768))\n",
    "\n",
    "prompt = \"{}, best quality, extremely detailed, clearness, naturalness, film grain, crystal clear, photo with color, actuality, <lora:add-detail-xl:1>\"\n",
    "negative_prompt = \"cartoon, anime, painting, disfigured, immature, blur, picture, 3D, render, semi-realistic, drawing, poorly drawn, bad anatomy, wrong anatomy, gray scale, worst quality, low quality, sketch\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    clip_image = load_demo_image(image_path=path, image_size=clip_image_size, device=device)\n",
    "    caption = clip_model.generate(clip_image, sample=False, num_beams=3, max_length=40, min_length=5)[0]\n",
    "    prompt_get = prompt.format(caption)\n",
    "\n",
    "    images = pipe(\n",
    "        init_image,\n",
    "        prompt=prompt_get,\n",
    "        negative_prompt=negative_prompt,\n",
    "        guidance_scale=7.0,\n",
    "        num_inference_steps=40,\n",
    "        noise_level=0,\n",
    "        generator=torch.manual_seed(42),\n",
    "    ).images\n",
    "\n",
    "# print(caption)\n",
    "display(init_image)\n",
    "display(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from diffusers import StableDiffusionXLUnclipImg2ImgPipeline\n",
    "\n",
    "# Start the StableUnCLIP Image variations pipeline\n",
    "device = \"cuda:3\"\n",
    "torch.cuda.set_device(torch.device(device))  # change allocation of current GPU\n",
    "# base_model_path = \"/data/noah/ckpt/finetuning/SDXL_UNCLIP_AD\"\n",
    "base_model_path = \"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/sdxl_unclip\"\n",
    "\n",
    "pipe = StableDiffusionXLUnclipImg2ImgPipeline.from_pretrained(base_model_path, torch_dtype=torch.float16)\n",
    "\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "path = \"/data/noah/inference/reimagine/input/2021-09-24-17-50-40_Front_1632473437550.png\"\n",
    "\n",
    "# Pipe to make the variation\n",
    "init_image = Image.open(path).convert(\"RGB\")\n",
    "init_image.thumbnail((1024, 1024))\n",
    "images = pipe(\n",
    "    init_image,\n",
    "    guidance_scale=11.0,\n",
    "    num_inference_steps=5,\n",
    "    generator=torch.manual_seed(42),\n",
    ").images\n",
    "\n",
    "\n",
    "# print(caption)\n",
    "display(init_image)\n",
    "display(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from diffusers.utils import load_image\n",
    "from diffusers import StableDiffusionXLReferencePipeline\n",
    "from diffusers.schedulers import UniPCMultistepScheduler\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "from diffusers.blip.models.blip import blip_decoder\n",
    "\n",
    "\n",
    "def load_demo_image(image_path, image_size, device):\n",
    "    raw_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    w, h = raw_image.size\n",
    "    # display(raw_image.resize((w//5,h//5)))\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        ]\n",
    "    )\n",
    "    image = transform(raw_image).unsqueeze(0).to(device)\n",
    "    return image\n",
    "\n",
    "\n",
    "device = \"cuda:3\"\n",
    "torch.cuda.set_device(torch.device(device))  # change allocation of current GPU\n",
    "\n",
    "path = \"/data/noah/inference/reimagine_exam/1657066114871_FR-View-CMR-Wide.png\"\n",
    "input_image = Image.open(path).convert(\"RGB\")\n",
    "input_image.thumbnail((1024, 1024))\n",
    "\n",
    "clip_image_size = 512\n",
    "clip_model_path = \"/data/noah/ckpt/pretrain_ckpt/BLIP/model_large_caption.pth\"\n",
    "clip_model = blip_decoder(pretrained=clip_model_path, image_size=clip_image_size, vit=\"large\")\n",
    "clip_model.eval()\n",
    "clip_model = clip_model.to(device)\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16).to(device)\n",
    "pipe = StableDiffusionXLReferencePipeline.from_pretrained(\n",
    "    \"/data/noah/ckpt/finetuning/SDXL_AD\",\n",
    "    # \"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/sdxl\",\n",
    "    vae=vae,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    ").to(device)\n",
    "\n",
    "pipe.load_lora_weights(\"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/lora_xl\", \"add-detail-xl.safetensors\")\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "prompt = \"{}, best quality, extremely detailed, clearness, naturalness, film grain, crystal clear, photo with color, actuality, <lora:add-detail-xl:3>\"\n",
    "negative_prompt = \"cartoon, anime, painting, disfigured, immature, blur, picture, 3D, render, semi-realistic, drawing, poorly drawn, bad anatomy, wrong anatomy, gray scale, worst quality, low quality, sketch\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    clip_image = load_demo_image(image_path=path, image_size=clip_image_size, device=device)\n",
    "    caption = clip_model.generate(clip_image, sample=False, num_beams=3, max_length=40, min_length=5)[0]\n",
    "    prompt_get = prompt.format(caption)\n",
    "\n",
    "    result_img = pipe(\n",
    "        ref_image=input_image,\n",
    "        prompt=prompt_get,\n",
    "        negative_prompt=negative_prompt,\n",
    "        guidance_scale=7.0,\n",
    "        num_inference_steps=20,\n",
    "        style_fidelity=0.5,\n",
    "        guidance_rescale=0.0,\n",
    "        reference_attn=True,\n",
    "        reference_adain=True,\n",
    "        generator=torch.manual_seed(42),\n",
    "    ).images[0]\n",
    "\n",
    "display(input_image)\n",
    "display(result_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from diffusers.utils import load_image\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "from diffusers.schedulers import UniPCMultistepScheduler\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "from diffusers.blip.models.blip import blip_decoder\n",
    "\n",
    "\n",
    "def load_demo_image(image_path, image_size, device):\n",
    "    raw_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    w, h = raw_image.size\n",
    "    # display(raw_image.resize((w//5,h//5)))\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        ]\n",
    "    )\n",
    "    image = transform(raw_image).unsqueeze(0).to(device)\n",
    "    return image\n",
    "\n",
    "\n",
    "device = \"cuda:3\"\n",
    "torch.cuda.set_device(torch.device(device))  # change allocation of current GPU\n",
    "\n",
    "path = \"/data/noah/inference/reimagine_exam/1657066114871_FR-View-CMR-Wide.png\"\n",
    "input_image = Image.open(path).convert(\"RGB\")\n",
    "input_image.thumbnail((1024, 1024))\n",
    "\n",
    "clip_image_size = 512\n",
    "clip_model_path = \"/data/noah/ckpt/pretrain_ckpt/BLIP/model_large_caption.pth\"\n",
    "clip_model = blip_decoder(pretrained=clip_model_path, image_size=clip_image_size, vit=\"large\")\n",
    "clip_model.eval()\n",
    "clip_model = clip_model.to(device)\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16).to(device)\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"/data/noah/ckpt/finetuning/SDXL_AD\",\n",
    "    vae=vae,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    ").to(device)\n",
    "pipe.load_lora_weights(\"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/lora_xl\", \"add-detail-xl.safetensors\")\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "prompt = \"{}, best quality, extremely detailed, clearness, naturalness, film grain, crystal clear, photo with color, actuality, <lora:add-detail-xl:1>\"\n",
    "negative_prompt = \"cartoon, anime, painting, disfigured, immature, blur, picture, 3D, render, semi-realistic, drawing, poorly drawn, bad anatomy, wrong anatomy, gray scale, worst quality, low quality, sketch\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    clip_image = load_demo_image(image_path=path, image_size=clip_image_size, device=device)\n",
    "    caption = clip_model.generate(clip_image, sample=False, num_beams=3, max_length=40, min_length=5)[0]\n",
    "    prompt_get = prompt.format(caption)\n",
    "\n",
    "    result_img = pipe(\n",
    "        prompt=prompt_get,\n",
    "        negative_prompt=negative_prompt,\n",
    "        guidance_scale=7.0,\n",
    "        num_inference_steps=20,\n",
    "        generator=torch.manual_seed(42),\n",
    "    ).images[0]\n",
    "\n",
    "display(input_image)\n",
    "display(result_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import SemanticStableDiffusionPipeline\n",
    "\n",
    "device = \"cuda:3\"\n",
    "torch.cuda.set_device(torch.device(device))  # change allocation of current GPU\n",
    "\n",
    "pipe = SemanticStableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "out = pipe(\n",
    "    prompt=\"/data/noah/ckpt/finetuning/SDXL_AD\",\n",
    "    num_images_per_prompt=1,\n",
    "    guidance_scale=7,\n",
    "    editing_prompt=[\"cars\" \"buildings\", \"trees\", \"road\"],\n",
    "    reverse_editing_direction=[\n",
    "        False,\n",
    "        False,\n",
    "        False,\n",
    "        False,\n",
    "    ],  # Direction of guidance i.e. increase all concepts\n",
    "    edit_warmup_steps=[10, 10, 10, 10],  # Warmup period for each concept\n",
    "    edit_guidance_scale=[4, 5, 5, 5.4],  # Guidance scale for each concept\n",
    "    edit_threshold=[\n",
    "        0.99,\n",
    "        0.975,\n",
    "        0.925,\n",
    "        0.96,\n",
    "    ],  # Threshold for each concept. Threshold equals the percentile of the latent space that will be discarded. I.e. threshold=0.99 uses 1% of the latent dimensions\n",
    "    edit_momentum_scale=0.3,  # Momentum scale that will be added to the latent guidance\n",
    "    edit_mom_beta=0.6,  # Momentum beta\n",
    "    edit_weights=[1, 1, 1, 1, 1],  # Weights of the individual concepts against each other\n",
    ")\n",
    "image = out.images[0]\n",
    "\n",
    "display(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
