{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Source Image and Mask of Rider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def mask_to_polygon(mask):\n",
    "    # 윤곽선 찾기\n",
    "    contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # 윤곽선을 다각형으로 변환\n",
    "    polygons = []\n",
    "    for contour in contours:\n",
    "        contour = contour.squeeze(axis=1)  # 차원 축소\n",
    "        polygon = contour[:, [0, 1]].tolist()  # (y, x) 순서로 변환하여 리스트로 저장\n",
    "        polygons.append(polygon)\n",
    "\n",
    "    return polygons\n",
    "\n",
    "\n",
    "def polygon_to_mask(mask, polygons, color=255):\n",
    "    polygons = np.array(polygons, dtype=np.int32)\n",
    "    state = False\n",
    "\n",
    "    try:\n",
    "        mask = cv2.fillPoly(mask.astype(\"uint8\"), [polygons], color)\n",
    "        state = True\n",
    "    except:\n",
    "        print(\"mask passed!\")\n",
    "\n",
    "    return mask, state\n",
    "\n",
    "\n",
    "def make_dirs(paths):\n",
    "    for path in paths:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def crop_from_mask(image, mask, padding=50):\n",
    "    # Find contours in the mask\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # If no contours found, return original image\n",
    "    if not contours:\n",
    "        return image\n",
    "\n",
    "    # Find the bounding box of the largest contour\n",
    "    x, y, w, h = cv2.boundingRect(contours[0])\n",
    "    \n",
    "    # padding\n",
    "    x = x-padding//2\n",
    "    y = y-padding//2\n",
    "    w = w+padding\n",
    "    h = h+padding\n",
    "    \n",
    "    x = 0 if x<0 else x\n",
    "    y = 0 if y<0 else y\n",
    "    \n",
    "    if x+w>image.shape[1]:\n",
    "        w = image.shape[1]-x\n",
    "\n",
    "    if y+h>image.shape[0]:\n",
    "        h = image.shape[0]-y\n",
    "\n",
    "    # Crop the image using the bounding box\n",
    "    cropped_image = image[y : y + h, x : x + w]\n",
    "    cropped_mask = mask[y : y + h, x : x + w]\n",
    "\n",
    "    return cropped_image, cropped_mask\n",
    "\n",
    "\n",
    "annotation_path = \"/data/noah/dataset/coco_rider/anno\"\n",
    "out_base_path = \"/data/noah/inference/magna_rider_premask\"\n",
    "out_mask_path = os.path.join(out_base_path, \"_masks\")\n",
    "out_image_path = os.path.join(out_base_path, \"_images\")\n",
    "\n",
    "make_dirs([out_base_path, out_mask_path, out_image_path])\n",
    "cnt = 0\n",
    "padding = 200\n",
    "threshold = 200 + padding\n",
    "\n",
    "for name in tqdm(os.listdir(annotation_path)):\n",
    "    ann_path = os.path.join(annotation_path, name)\n",
    "\n",
    "    with open(ann_path, \"r\") as f:\n",
    "        ann = json.load(f)\n",
    "\n",
    "    height, width = ann[\"metadata\"][\"height\"], ann[\"metadata\"][\"width\"]\n",
    "    mask = np.zeros((height, width))\n",
    "\n",
    "    for _ann in ann[\"annotations\"]:\n",
    "        points = _ann['points']\n",
    "        for point in points:\n",
    "            point = np.array(point, dtype=np.int32)\n",
    "            try:\n",
    "                mask = cv2.fillPoly(mask, [point], color=255)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    polygons = mask_to_polygon(mask)\n",
    "    annotations = []\n",
    "    image = Image.open(os.path.join(ann[\"parent_path\"], ann[\"filename\"]))\n",
    "\n",
    "    for polygon in polygons:\n",
    "        cnt += 1\n",
    "        mask = np.zeros((height, width))\n",
    "        mask, state = polygon_to_mask(mask, polygon)\n",
    "\n",
    "        if state:\n",
    "            crop_image, crop_mask = crop_from_mask(np.array(image).astype(\"uint8\"), mask.astype(\"uint8\"),padding=padding)\n",
    "            crop_image = Image.fromarray(crop_image)\n",
    "            crop_mask = Image.fromarray(crop_mask).convert(\"L\")\n",
    "            if crop_image.height < threshold:\n",
    "                continue\n",
    "\n",
    "            crop_image.save(os.path.join(out_image_path, \"{}.png\".format(cnt)))\n",
    "            crop_mask.save(os.path.join(out_mask_path, \"{}.png\".format(cnt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rider Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from controlnet_aux.processor import MidasDetector\n",
    "from diffusers import StableDiffusionControlNetInpaintPipeline, ControlNetModel, DDIMScheduler\n",
    "\n",
    "def make_dirs(paths):\n",
    "    for path in paths:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def make_grid(images, rows, cols):\n",
    "    w, h = images[0].size\n",
    "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
    "    for i, image in enumerate(images):\n",
    "        grid.paste(image, box=(i % cols * w, i // cols * h))\n",
    "    return grid\n",
    "\n",
    "def closest_multiple_of_8(number):\n",
    "    closest_multiple = (number // 8) * 8  # 가장 가까운 8의 배수\n",
    "    return closest_multiple\n",
    "\n",
    "device = 'cuda:3'\n",
    "instance_height = 1024\n",
    "k = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "image_dir = '/data/noah/inference/magna_rider_premask/images'\n",
    "mask_dir = '/data/noah/inference/magna_rider_premask/masks'\n",
    "out_image_dir = '/data/noah/inference/magna_object/rider/images'\n",
    "out_mask_dir = '/data/noah/inference/magna_object/rider/masks'\n",
    "make_dirs([out_image_dir, out_mask_dir])\n",
    "\n",
    "prompt_types = [\"a rider\", \"a motorcycle rider\", \"a bicycle rider\"]\n",
    "prompts = [\"{}, RAW photo, subject, 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3, <lora:add-detail:1>\".format(ptype) for ptype in prompt_types]\n",
    "negative_prompts = [\"(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime), blurry, text, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, UnrealisticDream\"]*len(prompt_types)\n",
    "num_inference_steps = 25\n",
    "guidance_scale = 7.5\n",
    "strength = 1.0\n",
    "sag_scale = 0.75\n",
    "controlnet_conditioning_scale = 0.75\n",
    "padding_mask_crop=0\n",
    "num_images_per_prompt=1\n",
    "\n",
    "model_id = \"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/rv_inpaint_5.1\"\n",
    "controlnet_id = \"/data/noah/ckpt/finetuning/controlnet_inpaint_coco_rider/checkpoint-21000/controlnet\"\n",
    "lora_id = \"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/lora_detail\"\n",
    "controlnet = ControlNetModel.from_pretrained(controlnet_id, torch_dtype=torch.float16)\n",
    "pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n",
    "    model_id, controlnet=controlnet, torch_dtype=torch.float16\n",
    ").to(device)\n",
    "pipe.load_lora_weights(lora_id, weight_name=\"add_detail.safetensors\")\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_freeu(s1=1.2, s2=0.5, b1=1.2, b2=1.4)\n",
    "\n",
    "\n",
    "midas = MidasDetector.from_pretrained(\"lllyasviel/Annotators\").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rider Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in tqdm(os.listdir(image_dir)):\n",
    "    image_path = os.path.join(image_dir, name)\n",
    "    mask_path = os.path.join(mask_dir, name)\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    mask = Image.open(mask_path)\n",
    "    \n",
    "    # generate condition image\n",
    "    con_image = midas(image, image_resolution=image.height)\n",
    "\n",
    "    height = closest_multiple_of_8(1024)\n",
    "    ratio = instance_height/image.height\n",
    "    width = closest_multiple_of_8(int(ratio*image.width))\n",
    "    \n",
    "    image = image.resize((width, height))\n",
    "    mask = mask.resize((width, height))\n",
    "    con_image = con_image.resize((width, height))\n",
    "\n",
    "    # mask boundary refinement\n",
    "    mask = np.array(mask)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, k, iterations=3)    \n",
    "    spots = np.argwhere(mask == 255)\n",
    "\n",
    "    image = np.array(image)\n",
    "    masked_image = np.ones((image.shape))*255\n",
    "\n",
    "    for spot in spots:\n",
    "        masked_image[spot[0], spot[1], :] = image[spot[0], spot[1], :]\n",
    "\n",
    "    mask = Image.fromarray(mask.astype('uint8')).convert('L')\n",
    "    masked_image = Image.fromarray(masked_image.astype('uint8'))\n",
    "    image = Image.fromarray(image.astype('uint8'))\n",
    "            \n",
    "    result_images = pipe(\n",
    "            prompt=prompts,\n",
    "            negative_prompt=negative_prompts,\n",
    "            image=masked_image,\n",
    "            control_image=con_image,\n",
    "            mask_image=mask,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            strength=strength,\n",
    "            sag_scale=sag_scale,\n",
    "            controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "            padding_mask_crop=padding_mask_crop,\n",
    "            num_images_per_prompt=num_images_per_prompt\n",
    "        ).images\n",
    "\n",
    "    for idx, result_image in enumerate(result_images):\n",
    "        prompt_type = prompt_types[idx]\n",
    "        output_image_path = os.path.join(out_image_dir, name[:-4]+'_{}'.format(prompt_type)+name[-4:])\n",
    "        output_mask_path = os.path.join(out_mask_dir, name[:-4]+'_{}'.format(prompt_type)+name[-4:])\n",
    "        result_image.save(output_image_path)\n",
    "        mask.save(output_mask_path)\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
