{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Video Diffusion Video to Video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import gc\n",
    "import torch\n",
    "from diffusers import (\n",
    "    StableDiffusionControlNetPipeline,\n",
    "    StableVideoDiffusionPipeline,\n",
    "    ControlNetModel,\n",
    ")\n",
    "from diffusers.utils import export_to_gif, make_image_grid\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "from controlnet_aux import CannyDetector\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from tqdm import tqdm\n",
    "from diffusers.blip import blip_decoder\n",
    "from IPython.display import display, Image\n",
    "\n",
    "input_path = \"/data/noah/inference/simulation/svd_test/input\"\n",
    "output_path = \"/data/noah/inference/simulation/svd_test/output/output.gif\"\n",
    "device = \"cuda:0\"\n",
    "vid_device = \"cuda:1\"\n",
    "num_frames = 12\n",
    "height = 512\n",
    "width = 768\n",
    "# torch.cuda.set_device(device)\n",
    "generator = torch.Generator().manual_seed(100)\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_frame(video_path):\n",
    "    images = []\n",
    "    edges = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    canny = CannyDetector()\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = PILImage.fromarray(frame)\n",
    "\n",
    "        images.append(frame)\n",
    "\n",
    "        edges.append(\n",
    "            canny(\n",
    "                frame,\n",
    "                detect_resolution=frame.height,\n",
    "                image_resolution=frame.height,\n",
    "                low_threshold=100,\n",
    "                high_threshold=200,\n",
    "            )\n",
    "        )\n",
    "    cap.release()\n",
    "    return images, edges\n",
    "\n",
    "\n",
    "def load_demo_image(image, image_size, device):\n",
    "    w, h = image.size\n",
    "    # display(raw_image.resize((w//5,h//5)))\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        ]\n",
    "    )\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    return image\n",
    "\n",
    "\n",
    "model = blip_decoder(\n",
    "    pretrained=\"/data/noah/ckpt/pretrain_ckpt/BLIP/model_large_caption.pth\", image_size=512, vit=\"large\"\n",
    ")\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "canny = CannyDetector()\n",
    "\n",
    "# Load the motion adapter\n",
    "model_id = \"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/rv\"\n",
    "lora_id = \"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/lora_detail\"\n",
    "lora_name = \"add_detail.safetensors\"\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\"/data/noah/ckpt/finetuning/Control_SD_AD\", torch_dtype=torch.float16).to(\n",
    "    device\n",
    ")\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "pipe.enable_freeu(s1=0.9, s2=0.2, b1=1.2, b2=1.4)\n",
    "pipe.load_lora_weights(lora_id, weight_name=lora_name)\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "vid_pipe = StableVideoDiffusionPipeline.from_pretrained(\n",
    "    \"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/svd_xt\", torch_dtype=torch.float16\n",
    ")\n",
    "vid_pipe = vid_pipe.to(vid_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_images = []\n",
    "input_images = []\n",
    "control_images = []\n",
    "\n",
    "result_vid_images = []\n",
    "input_vid_images = []\n",
    "control_vid_images = []\n",
    "\n",
    "for video_name in os.listdir(input_path):\n",
    "    video_path = os.path.join(input_path, video_name)\n",
    "    images, edges = extract_frame(video_path)[:num_frames]\n",
    "    blip_image = load_demo_image(image=images[0], image_size=height, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        caption = model.generate(blip_image, sample=True, top_p=0.9, max_length=20, min_length=5)[0]\n",
    "        print(caption)\n",
    "\n",
    "    input_images.append(images[0])\n",
    "    control_images.append(edges[0])\n",
    "    input_vid_images.append(images)\n",
    "    control_vid_images.append(edges)\n",
    "\n",
    "    images = [image.resize((width, height)) for image in images]\n",
    "\n",
    "    prompt = \"{}, outdoor, best quality, extremely detailed, clearness, naturalness, film grain, crystal clear, photo with color, actuality, <lora:add-detail:-1>\".format(\n",
    "        caption\n",
    "    )\n",
    "    negative_prompt = \"cartoon, anime, painting, disfigured, immature, blur, picture, 3D, render, semi-realistic, drawing, poorly drawn, bad anatomy, wrong anatomy, gray scale, worst quality, low quality, sketch\"\n",
    "\n",
    "    result_image = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        image=edges[0],\n",
    "        height=height,  # 512\n",
    "        width=width,  # 768\n",
    "        guidance_scale=7,  # 7\n",
    "        num_inference_steps=40,  # 20\n",
    "        controlnet_conditioning_scale=0.75,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "\n",
    "    frames = vid_pipe(\n",
    "        image=result_image,\n",
    "        height=height,  # 512\n",
    "        width=width,  # 768\n",
    "        num_frames=len(images),\n",
    "        num_inference_steps=25,\n",
    "        decode_chunk_size=8,\n",
    "        motion_bucket_id=230,\n",
    "        noise_aug_strength=0,\n",
    "        min_guidance_scale=1.0,\n",
    "        max_guidance_scale=3.0,\n",
    "        generator=generator,\n",
    "    ).frames[0]\n",
    "\n",
    "    new_w, new_h = edges[0].width, edges[0].height\n",
    "    result_image = result_image.resize((new_w, new_h))\n",
    "    result_images.append(result_image)\n",
    "\n",
    "    for idx, f in enumerate(frames):\n",
    "        frames[idx] = f.resize((new_w, new_h))\n",
    "    result_vid_images.append(frames)\n",
    "\n",
    "image_grid = make_image_grid(input_images, rows=1, cols=len(input_images))\n",
    "control_grid = make_image_grid(control_images, rows=1, cols=len(input_images))\n",
    "result_grid = make_image_grid(result_images, rows=1, cols=len(input_images))\n",
    "result_grid = make_image_grid([image_grid, control_grid, result_grid], rows=3, cols=1)\n",
    "display(result_grid)\n",
    "\n",
    "grids = []\n",
    "result_vid_images = np.array(result_vid_images).astype(\"uint8\")\n",
    "input_vid_images = np.array(input_vid_images).astype(\"uint8\")\n",
    "control_vid_images = np.array(control_vid_images).astype(\"uint8\")\n",
    "num_video = len(os.listdir(input_path))\n",
    "\n",
    "for idx in range(num_frames):\n",
    "    sub_result_images = [PILImage.fromarray(r) for r in result_vid_images[:, idx, ...]]\n",
    "    sub_input_images = [PILImage.fromarray(i) for i in input_vid_images[:, idx, ...]]\n",
    "    sub_control_images = [PILImage.fromarray(c) for c in control_vid_images[:, idx, ...]]\n",
    "\n",
    "    image_grid = make_image_grid(sub_input_images, rows=1, cols=num_video)\n",
    "    result_grid = make_image_grid(sub_result_images, rows=1, cols=num_video)\n",
    "    grid = make_image_grid([image_grid, result_grid], rows=2, cols=1)\n",
    "    grids.append(grid.resize((grid.width // 2, grid.height // 2)))\n",
    "\n",
    "export_to_gif(grids, output_path)\n",
    "display(Image(output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Video Diffusion ControlNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import gc\n",
    "import torch\n",
    "from diffusers import (\n",
    "    StableDiffusionControlNetPipeline,\n",
    "    StableVideoDiffusionControlNetPipeline,\n",
    "    ControlNetSVDModel,\n",
    "    ControlNetModel,\n",
    ")\n",
    "from diffusers.utils import export_to_gif, make_image_grid\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "from controlnet_aux import CannyDetector\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from tqdm import tqdm\n",
    "from diffusers.blip import blip_decoder\n",
    "from IPython.display import display, Image\n",
    "\n",
    "input_path = \"/data/noah/inference/simulation/svd_test/input\"\n",
    "output_path = \"/data/noah/inference/simulation/svd_test/output/output.gif\"\n",
    "device = \"cuda:2\"\n",
    "num_frames = 8\n",
    "height = 512\n",
    "width = 768\n",
    "# torch.cuda.set_device(device)\n",
    "generator = torch.Generator().manual_seed(100)\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_frame(video_path):\n",
    "    images = []\n",
    "    edges = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    canny = CannyDetector()\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = PILImage.fromarray(frame)\n",
    "\n",
    "        images.append(frame)\n",
    "\n",
    "        edges.append(\n",
    "            canny(\n",
    "                frame,\n",
    "                detect_resolution=frame.height,\n",
    "                image_resolution=frame.height,\n",
    "                low_threshold=50,\n",
    "                high_threshold=100,\n",
    "            )\n",
    "        )\n",
    "    cap.release()\n",
    "    return images, edges\n",
    "\n",
    "\n",
    "def load_demo_image(image, image_size, device):\n",
    "    w, h = image.size\n",
    "    # display(raw_image.resize((w//5,h//5)))\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        ]\n",
    "    )\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    return image\n",
    "\n",
    "\n",
    "model = blip_decoder(\n",
    "    pretrained=\"/data/noah/ckpt/pretrain_ckpt/BLIP/model_large_caption.pth\", image_size=512, vit=\"large\"\n",
    ")\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "canny = CannyDetector()\n",
    "\n",
    "model_id = \"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/rv\"\n",
    "lora_id = \"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/lora_detail\"\n",
    "lora_name = \"add_detail.safetensors\"\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\"/data/noah/ckpt/finetuning/Control_SD_AD\", torch_dtype=torch.float16).to(\n",
    "    device\n",
    ")\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "pipe.enable_freeu(s1=0.9, s2=0.2, b1=1.2, b2=1.4)\n",
    "pipe.load_lora_weights(lora_id, weight_name=lora_name)\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "result_images = []\n",
    "input_images = []\n",
    "control_images = []\n",
    "\n",
    "result_vid_images = []\n",
    "input_vid_images = []\n",
    "control_vid_images = []\n",
    "\n",
    "for video_name in os.listdir(input_path):\n",
    "    video_path = os.path.join(input_path, video_name)\n",
    "    images, edges = extract_frame(video_path)[:num_frames]\n",
    "    blip_image = load_demo_image(image=images[0], image_size=height, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        caption = model.generate(blip_image, sample=True, top_p=0.9, max_length=20, min_length=5)[0]\n",
    "        print(caption)\n",
    "\n",
    "    input_images.append(images[0])\n",
    "    control_images.append(edges[0])\n",
    "    input_vid_images.append(images)\n",
    "    control_vid_images.append(edges)\n",
    "\n",
    "    images = [image.resize((width, height)) for image in images]\n",
    "\n",
    "    prompt = \"{}, outdoor, best quality, extremely detailed, clearness, naturalness, film grain, crystal clear, photo with color, actuality, <lora:add-detail:-1>\".format(\n",
    "        caption\n",
    "    )\n",
    "    negative_prompt = \"cartoon, anime, painting, disfigured, immature, blur, picture, 3D, render, semi-realistic, drawing, poorly drawn, bad anatomy, wrong anatomy, gray scale, worst quality, low quality, sketch\"\n",
    "\n",
    "    result_image = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        image=edges[0],\n",
    "        height=height,  # 512\n",
    "        width=width,  # 768\n",
    "        guidance_scale=6.5,  # 7\n",
    "        num_inference_steps=40,  # 20\n",
    "        controlnet_conditioning_scale=0.75,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "\n",
    "    new_w, new_h = edges[0].width, edges[0].height\n",
    "    result_image = result_image.resize((new_w, new_h))\n",
    "    result_images.append(result_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet_svd = ControlNetSVDModel.from_pretrained(\n",
    "    \"/data/noah/ckpt/finetuning/SVD_CON_AD_SEQ/checkpoint-40000/controlnet\", torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "vid_pipe = StableVideoDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"/data/noah/ckpt/finetuning/SVD_CON_AD_SEQ\", controlnet=controlnet_svd, torch_dtype=torch.float16\n",
    ")\n",
    "vid_pipe = vid_pipe.to(device)\n",
    "\n",
    "for idx, control_frames in enumerate(control_vid_images):\n",
    "    frames = vid_pipe(\n",
    "        image=result_images[idx].resize((width, height)),\n",
    "        controlnet_condition=control_frames,\n",
    "        height=height,  # 512\n",
    "        width=width,  # 768\n",
    "        num_frames=len(control_frames),\n",
    "        num_inference_steps=25,\n",
    "        min_guidance_scale=1.0,\n",
    "        max_guidance_scale=3.0,\n",
    "        motion_bucket_id=250,\n",
    "        noise_aug_strength=0,\n",
    "        controlnet_cond_scale=1.0,\n",
    "        # decode_chunk_size=8,\n",
    "        decode_chunk_size=1,\n",
    "        generator=generator,\n",
    "    ).frames[0]\n",
    "\n",
    "    new_w, new_h = result_images[idx].width, result_images[idx].height\n",
    "    for ind, f in enumerate(frames):\n",
    "        frames[ind] = f.resize((new_w, new_h))\n",
    "\n",
    "    result_vid_images.append(frames)\n",
    "\n",
    "image_grid = make_image_grid(input_images, rows=1, cols=len(input_images))\n",
    "control_grid = make_image_grid(control_images, rows=1, cols=len(input_images))\n",
    "result_grid = make_image_grid(result_images, rows=1, cols=len(input_images))\n",
    "result_grid = make_image_grid([image_grid, control_grid, result_grid], rows=3, cols=1)\n",
    "display(result_grid)\n",
    "\n",
    "grids = []\n",
    "result_vid_images = np.array(result_vid_images).astype(\"uint8\")\n",
    "input_vid_images = np.array(input_vid_images).astype(\"uint8\")\n",
    "control_vid_images = np.array(control_vid_images).astype(\"uint8\")\n",
    "num_video = len(os.listdir(input_path))\n",
    "\n",
    "for idx in range(num_frames):\n",
    "    sub_result_images = [PILImage.fromarray(r) for r in result_vid_images[:, idx, ...]]\n",
    "    sub_input_images = [PILImage.fromarray(i) for i in input_vid_images[:, idx, ...]]\n",
    "    sub_control_images = [PILImage.fromarray(c) for c in control_vid_images[:, idx, ...]]\n",
    "\n",
    "    image_grid = make_image_grid(sub_input_images, rows=1, cols=num_video)\n",
    "    condition_grid = make_image_grid(sub_control_images, rows=1, cols=num_video)\n",
    "    result_grid = make_image_grid(sub_result_images, rows=1, cols=num_video)\n",
    "    grid = make_image_grid([image_grid, condition_grid, result_grid], rows=3, cols=1)\n",
    "    grids.append(grid.resize((grid.width // 2, grid.height // 2)))\n",
    "\n",
    "export_to_gif(grids, output_path)\n",
    "display(Image(output_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
