{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIMMO SIM2REAL INFERENCE DEMO\n",
    "\n",
    "**It aims to convert simulated images into real-life images**\n",
    "\n",
    "**1. Load the Input Image and Setting Parameter**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from diffusers import (\n",
    "    StableDiffusionImg2ImgPipeline,\n",
    "    StableDiffusionXLImg2ImgPipeline,\n",
    "    StableDiffusionControlNetPipeline,\n",
    "    StableDiffusionXLControlNetPipeline,\n",
    "    ControlNetModel,\n",
    "    UniPCMultistepScheduler,\n",
    "    AutoencoderKL,\n",
    ")\n",
    "\n",
    "device = \"cuda:3\"\n",
    "torch.cuda.set_device(torch.device(device))  # change allocation of current GPU\n",
    "\n",
    "\n",
    "def inference_single(\n",
    "    model_id, prompt, negative_prompt, image, device, lora_id=None, lora_name=None, mode=\"SD\", control_model_id=None\n",
    "):\n",
    "    if mode == \"SD\":\n",
    "        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "            model_id, torch_dtype=torch.float16, use_safetensors=True\n",
    "        ).to(device)\n",
    "    elif mode == \"SDXL\":\n",
    "        pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "            model_id, torch_dtype=torch.float16, use_safetensors=True\n",
    "        ).to(device)\n",
    "    elif mode == \"Control_SD\":\n",
    "        if control_model_id is None:\n",
    "            controlnet = ControlNetModel.from_pretrained(\n",
    "                \"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16, use_safetensors=True\n",
    "            ).to(device)\n",
    "        else:\n",
    "            controlnet = ControlNetModel.from_pretrained(\n",
    "                control_model_id, torch_dtype=torch.float16, use_safetensors=True\n",
    "            ).to(device)\n",
    "\n",
    "        pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "            model_id, controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True\n",
    "        ).to(device)\n",
    "        pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "        # pipe.enable_model_cpu_offload()\n",
    "\n",
    "    elif mode == \"Control_SDXL\":\n",
    "        if control_model_id is None:\n",
    "            controlnet = ControlNetModel.from_pretrained(\n",
    "                \"diffusers/controlnet-canny-sdxl-1.0\", torch_dtype=torch.float16, use_safetensors=True\n",
    "            ).to(device)\n",
    "        else:\n",
    "            controlnet = ControlNetModel.from_pretrained(\n",
    "                control_model_id, torch_dtype=torch.float16, use_safetensors=True\n",
    "            ).to(device)\n",
    "\n",
    "        vae = AutoencoderKL.from_pretrained(\n",
    "            \"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16, use_safetensoƒrs=True\n",
    "        ).to(device)\n",
    "        pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "            model_id, controlnet=controlnet, vae=vae, torch_dtype=torch.float16, use_safetensors=True\n",
    "        ).to(device)\n",
    "        pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "        # pipe.enable_model_cpu_offload()\n",
    "    else:\n",
    "        raise ValueError('model must be in [\"SD\", \"SDXL\", \"ControlNet_SD\", \"ControlNet_SDXL\"]')\n",
    "\n",
    "    if lora_id and lora_name:\n",
    "        pipe.load_lora_weights(lora_id, weight_name=lora_name)\n",
    "        pipe.to(device)\n",
    "\n",
    "        if mode.startswith(\"Control\"):\n",
    "            image = pipe(\n",
    "                prompt=prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                image=image,\n",
    "                guidance_scale=8.0,\n",
    "                num_inference_steps=50,\n",
    "                generator=torch.manual_seed(123),\n",
    "                cross_attention_kwargs={\"scale\": 0.5},\n",
    "            ).images[0]\n",
    "        else:\n",
    "            image = pipe(\n",
    "                prompt=prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                image=image,\n",
    "                strength=0.5,\n",
    "                guidance_scale=8.0,\n",
    "                num_inference_steps=50,\n",
    "                generator=torch.manual_seed(123),\n",
    "                cross_attention_kwargs={\"scale\": 0.5},\n",
    "            ).images[0]\n",
    "\n",
    "    else:\n",
    "        if mode.startswith(\"Control\"):\n",
    "            image = pipe(\n",
    "                prompt=prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                image=image,\n",
    "                guidance_scale=8.0,\n",
    "                num_inference_steps=50,\n",
    "                generator=torch.manual_seed(123),\n",
    "            ).images[0]\n",
    "        else:\n",
    "            image = pipe(\n",
    "                prompt=prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                image=image,\n",
    "                strength=0.5,\n",
    "                guidance_scale=8.0,\n",
    "                num_inference_steps=50,\n",
    "                generator=torch.manual_seed(123),\n",
    "            ).images[0]\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "# prompt = \"A man is driving in the car, indoor, best quality, extremely detailed, clearness, naturalness, film grain, crystal clear, photo with color, actuality\"\n",
    "\n",
    "# sample_6.png\n",
    "# prompt = \"a black car driving down a street next to a bridge, outdoor, best quality, extremely detailed, clearness, naturalness, film grain, crystal clear, photo with color, actuality\"\n",
    "\n",
    "# img_3.png\n",
    "# prompt = \"a yellow car driving down a street next to tall buildings, outdoor, best quality, extremely detailed, clearness, naturalness, film grain, crystal clear, photo with color, actuality\"\n",
    "\n",
    "# /data/noah/dataset/gta2cityscapes/A/00036.png\n",
    "# prompt = \"a truck driving down a highway next to a car, outdoor, best quality, extremely detailed, clearness, naturalness, film grain, crystal clear, photo with color, actuality\"\n",
    "\n",
    "# /data/noah/inference/gta2cityscapes/00086.png\n",
    "# prompt = \"a car driving down a street next to palm trees, outdoor, best quality, extremely detailed, clearness, naturalness, film grain, crystal clear, photo with color, actuality\"\n",
    "\n",
    "# /data/noah/dataset/gta2cityscapes/A/00065.png\n",
    "# prompt = \"a car driving down a city street at night, outdoor, best quality, extremely detailed, clearness, naturalness, film grain, crystal clear, photo with color, actuality\"\n",
    "\n",
    "# /data/noah/dataset/gta2cityscapes/A/00348.png\n",
    "# prompt = \"a a red truck driving down a street next to tall building, outdoor, best quality, extremely detailed, clearness, naturalness, film grain, crystal clear, photo with color, actuality\"\n",
    "\n",
    "prompt = \"a car driving\"\n",
    "negative_prompt = \"cartoon, anime, painting, disfigured, immature, blur, picture, 3D, render, semi-realistic, drawing, poorly drawn, bad anatomy, wrong anatomy, gray scale, worst quality, low quality, sketch\"\n",
    "\n",
    "origin = Image.open(\"/data/noah/inference/reimagine/input/2021-09-24-17-50-40_Front_1632473437550.png\").convert(\"RGB\")\n",
    "init_image = origin.resize((768, 512))\n",
    "\n",
    "init_image.save(\"./1.init_image.jpg\")\n",
    "\n",
    "init_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Load the Stable Diffusion v1.5 model**\n",
    "\n",
    "here is the Stable Diffusion [link](https://huggingface.co/CompVis/stable-diffusion-v1-4/tree/main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "image = inference_single(model_id, prompt, negative_prompt, init_image, device)\n",
    "image.save(\"./2.Stable Diffusion.jpg\")\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Load the Realistic Vision model based on Stable Diffusion v1.5**\n",
    "\n",
    "**here is the Realistic Vision [link](https://civitai.com/models/4201/realistic-vision-v20)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/rv\"\n",
    "image = inference_single(model_id, prompt, negative_prompt, init_image, device)\n",
    "image.save(\"./3.Realistic Vision.jpg\")\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Load the ControlNet based on Realistic Vision**\n",
    "\n",
    "**here is the detail information [link](https://huggingface.co/docs/diffusers/v0.21.0/en/training/controlnet) about training ControlNet and [link](https://huggingface.co/docs/diffusers/v0.21.0/en/using-diffusers/controlnet#multicontrolnet) about inference ControlNet**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "model_id = \"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/rv\"\n",
    "control_model_id = \"/data/noah/ckpt/pretrain_ckpt/ControlNet/sd/\"\n",
    "\n",
    "\n",
    "canny_image = cv2.Canny(np.array(init_image), low_threshold, high_threshold)\n",
    "canny_image = canny_image[:, :, None]\n",
    "canny_image = np.concatenate([canny_image, canny_image, canny_image], axis=2)\n",
    "canny_image = Image.fromarray(canny_image)\n",
    "\n",
    "image = inference_single(\n",
    "    model_id, prompt, negative_prompt, canny_image, device, mode=\"Control_SD\", control_model_id=control_model_id\n",
    ")\n",
    "image.save(\"./4.ControlNet(SD)+RV.jpg\")\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Load Fine Tuned Realistic Vision Model**\n",
    "\n",
    "here is the detail information [link](https://huggingface.co/docs/diffusers/training/text2image) about training text2img process\n",
    "\n",
    "if training on your own dataset, first generate dataset with image and text created by image captioning([BLIP](https://github.com/salesforce/BLIP))\n",
    "\n",
    "and then, train txt2img model(Realistic Vision) with [train script](https://github.com/huggingface/diffusers/tree/main/examples/text_to_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"/data/noah/ckpt/finetuning/Realistic_AD\"\n",
    "image = inference_single(model_id, prompt, negative_prompt, init_image, device)\n",
    "image.save(\"./5.Fine Tuning(Realistic Vision).jpg\")\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Load the LoRA model based on Detail Tweaker**\n",
    "\n",
    "**here is the detail tweaker [link](https://civitai.com/models/58390/detail-tweaker-lora-lora)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model_id = \"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/rv\"\n",
    "# model_id = \"/data/noah/ckpt/finetuning/Realistic_AD\"\n",
    "# lora_id = \"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/lora/\"\n",
    "# lora_name = \"add_detail.safetensors\"\n",
    "# image = inference_single(model_id, prompt + \", <lora:add_detail:1>\", negative_prompt, init_image, device, lora_id=lora_id, lora_name=lora_name)\n",
    "# image.save('./6.Fine Tuning(Realistic Vision) + Detail Tweaker.jpg')\n",
    "# image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Load the Fine Tuned Realistic Vision v5.1 based ControlNet**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "model_id = \"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/rv\"\n",
    "control_model_id = \"/data/noah/ckpt/finetuning/Control_RV_AD_prompt/checkpoint-25000/controlnet\"\n",
    "\n",
    "canny_image = cv2.Canny(np.array(init_image), low_threshold, high_threshold)\n",
    "canny_image = canny_image[:, :, None]\n",
    "canny_image = np.concatenate([canny_image, canny_image, canny_image], axis=2)\n",
    "canny_image = Image.fromarray(canny_image)\n",
    "\n",
    "image = inference_single(\n",
    "    model_id, prompt, negative_prompt, canny_image, device, mode=\"Control_SD\", control_model_id=control_model_id\n",
    ")\n",
    "image.save(\"./7.Fine Tuning ControlNet(SD)+RV.jpg\")\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Load the Realistic Vision XL based on SDXL**\n",
    "\n",
    "here is the Realistic Vision XL [link](https://civitai.com/models/139562?modelVersionId=154590)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_image = origin.resize((768, 768))\n",
    "\n",
    "model_id = \"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/rvxl_v2\"\n",
    "image = inference_single(model_id, prompt, negative_prompt, init_image, device, mode=\"SDXL\")\n",
    "image.save(\"./8.Realistic Vision XL.jpg\")\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Load the Fine Tuned Realistic Vision XL Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"/data/noah/ckpt/finetuning/Realistic_XL_AD/\"\n",
    "# image = inference_single(model_id, prompt,negative_prompt,init_image, device, mode='SDXL')\n",
    "# image.save('./9.Fine Tuning(Realistic Vision XL).jpg')\n",
    "# image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Load the ControlNet with Realistic Vision XL**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "init_image = origin.resize((1024, 1024))\n",
    "model_id = \"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/rvxl_v2\"\n",
    "control_model_id = \"/data/noah/ckpt/pretrain_ckpt/ControlNet/sdxl\"\n",
    "\n",
    "canny_image = cv2.Canny(np.array(init_image), low_threshold, high_threshold)\n",
    "canny_image = canny_image[:, :, None]\n",
    "canny_image = np.concatenate([canny_image, canny_image, canny_image], axis=2)\n",
    "canny_image = Image.fromarray(canny_image)\n",
    "\n",
    "image = inference_single(\n",
    "    model_id, prompt, negative_prompt, canny_image, device, mode=\"Control_SDXL\", control_model_id=control_model_id\n",
    ")\n",
    "image.save(\"./10.ControlNet(SDXL)_RVXL.jpg\")\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11. Load the FineTuned ControlNet with Realistic Vision XL**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "\n",
    "origin.thumbnail((1024, 1024))\n",
    "\n",
    "# model_id = 'stabilityai/stable-diffusion-xl-base-1.0'\n",
    "model_id = \"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/sdxl\"\n",
    "control_model_id = \"/data/noah/ckpt/finetuning/Control_SDXL_AD/controlnet_70k\"\n",
    "\n",
    "canny_image = cv2.Canny(np.array(origin), low_threshold, high_threshold)\n",
    "canny_image = canny_image[:, :, None]\n",
    "canny_image = np.concatenate([canny_image, canny_image, canny_image], axis=2)\n",
    "canny_image = Image.fromarray(canny_image)\n",
    "\n",
    "image = inference_single(\n",
    "    model_id, prompt, negative_prompt, canny_image, device, mode=\"Control_SDXL\", control_model_id=control_model_id\n",
    ")\n",
    "image.save(\"./11.Fine Tuning(ControlNet + Realistic Vision).jpg.jpg\")\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12. Batch Processing of Fine Tuned Stable Diffusion XL v1.0 + ControlNet and Inference With Realistic Vision v2.0**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "from diffusers import (\n",
    "    StableDiffusionImg2ImgPipeline,\n",
    "    StableDiffusionXLImg2ImgPipeline,\n",
    "    StableDiffusionControlNetPipeline,\n",
    "    StableDiffusionXLControlNetPipeline,\n",
    "    StableDiffusionXLControlNetImg2ImgPipeline,\n",
    "    ControlNetModel,\n",
    "    UniPCMultistepScheduler,\n",
    "    AutoencoderKL,\n",
    ")\n",
    "from diffusers.blip.models.blip import blip_decoder\n",
    "\n",
    "\n",
    "def load_demo_image(image_path, image_size, device):\n",
    "    raw_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    w, h = raw_image.size\n",
    "    # display(raw_image.resize((w//5,h//5)))\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        ]\n",
    "    )\n",
    "    image = transform(raw_image).unsqueeze(0).to(device)\n",
    "    return image\n",
    "\n",
    "\n",
    "def generate_pipeline(model_id, device, mode=\"SD\", control_model_id=None):\n",
    "    if mode == \"SD\":\n",
    "        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "            model_id, torch_dtype=torch.float16, use_safetensors=True\n",
    "        ).to(device)\n",
    "    elif mode == \"SDXL\":\n",
    "        pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "            model_id, torch_dtype=torch.float16, use_safetensors=True\n",
    "        ).to(device)\n",
    "    elif mode == \"Control_SD\":\n",
    "        if control_model_id is None:\n",
    "            controlnet = ControlNetModel.from_pretrained(\n",
    "                \"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16, use_safetensors=True\n",
    "            ).to(device)\n",
    "        else:\n",
    "            controlnet = ControlNetModel.from_pretrained(\n",
    "                control_model_id, torch_dtype=torch.float16, use_safetensors=True\n",
    "            ).to(device)\n",
    "\n",
    "        pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "            model_id, controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True\n",
    "        ).to(device)\n",
    "        pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "        # pipe.enable_model_cpu_offload()\n",
    "\n",
    "    elif mode == \"Control_SDXL\":\n",
    "        if control_model_id is None:\n",
    "            controlnet = ControlNetModel.from_pretrained(\n",
    "                \"diffusers/controlnet-canny-sdxl-1.0\", torch_dtype=torch.float16, use_safetensors=True\n",
    "            ).to(device)\n",
    "        else:\n",
    "            controlnet = ControlNetModel.from_pretrained(\n",
    "                control_model_id, torch_dtype=torch.float16, use_safetensors=True\n",
    "            ).to(device)\n",
    "\n",
    "        vae = AutoencoderKL.from_pretrained(\n",
    "            \"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16, use_safetensoƒrs=True\n",
    "        ).to(device)\n",
    "        pipe = StableDiffusionXLControlNetImg2ImgPipeline.from_pretrained(\n",
    "            model_id, controlnet=controlnet, vae=vae, torch_dtype=torch.float16, use_safetensors=True\n",
    "        ).to(device)\n",
    "        pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "        # pipe.enable_model_cpu_offload()\n",
    "    else:\n",
    "        raise ValueError('model must be in [\"SD\", \"SDXL\", \"ControlNet_SD\", \"ControlNet_SDXL\"]')\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def inference_sim2real(pipe, prompt, negative_prompt, image, control_image, mode):\n",
    "    if mode.startswith(\"Control\"):\n",
    "        image = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            image=image,\n",
    "            control_image=control_image,\n",
    "            guidance_scale=8.0,\n",
    "            num_inference_steps=50,\n",
    "            generator=torch.manual_seed(123),\n",
    "        ).images[0]\n",
    "    else:\n",
    "        image = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            image=image,\n",
    "            strength=0.5,\n",
    "            guidance_scale=8.0,\n",
    "            num_inference_steps=50,\n",
    "            generator=torch.manual_seed(123),\n",
    "        ).images[0]\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "# clip_device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "# clip_image_size = 512\n",
    "# clip_model_path = \"/data/noah/ckpt/pretrain_ckpt/BLIP/model_large_caption.pth\"\n",
    "# clip_model = blip_decoder(pretrained=clip_model_path, image_size=clip_image_size, vit=\"large\")\n",
    "# clip_model.eval()\n",
    "# clip_model = clip_model.to(clip_device)\n",
    "\n",
    "sim2real_device = \"cuda:3\"\n",
    "sim2real_mode = \"Control_SDXL\"\n",
    "sim2real_model_id = \"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/sdxl\"\n",
    "sim2real_control_model_id = \"/data/noah/ckpt/finetuning/Control_SDXL_AD/controlnet_70k\"\n",
    "sim2real_pipe = generate_pipeline(\n",
    "    sim2real_model_id, sim2real_device, mode=sim2real_mode, control_model_id=sim2real_control_model_id\n",
    ")\n",
    "\n",
    "image_dir = \"/data/noah/inference/sim2real/_input\"\n",
    "out_dir = \"/data/noah/inference/sim2real/output\"\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "prompt = \"{} ,outdoor, best quality, extremely detailed, clearness, naturalness, film grain, crystal clear, photo with color, actuality\"\n",
    "negative_prompt = \"cartoon, anime, painting, disfigured, immature, blur, picture, 3D, render, semi-realistic, drawing, poorly drawn, bad anatomy, wrong anatomy, gray scale, worst quality, low quality, sketch\"\n",
    "\n",
    "\n",
    "for _ in os.listdir(image_dir):\n",
    "    image_path = os.path.join(image_dir, _)\n",
    "    # clip_image = load_demo_image(image_path=image_dir, image_size=clip_image_size, device=device)\n",
    "    sim2real_image = Image.open(image_path).convert(\"RGB\")\n",
    "    sim2real_image = sim2real_image.resize((1024, 1024))\n",
    "    sim2real_canny_image = cv2.Canny(np.array(sim2real_image), low_threshold, high_threshold)\n",
    "    sim2real_canny_image = sim2real_canny_image[:, :, None]\n",
    "    sim2real_canny_image = np.concatenate([sim2real_canny_image, sim2real_canny_image, sim2real_canny_image], axis=2)\n",
    "    sim2real_canny_image = Image.fromarray(sim2real_canny_image)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # caption = clip_model.generate(clip_image, sample=False, num_beams=3, max_length=40, min_length=5)[0]\n",
    "        # caption = clip_model.generate(image, sample=True, top_p=0.9, max_length=20, min_length=5)[0]\n",
    "\n",
    "        sim2real_result_image = inference_sim2real(\n",
    "            sim2real_pipe,\n",
    "            prompt.format(\"cars are driving on the road\"),\n",
    "            negative_prompt,\n",
    "            image=sim2real_image,\n",
    "            control_image=sim2real_canny_image,\n",
    "            mode=sim2real_mode,\n",
    "        )\n",
    "\n",
    "    sim2real_result_image.save(os.path.join(out_dir, _))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "from controlnet_aux.lineart import LineartDetector\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "from diffusers import (\n",
    "    StableDiffusionImg2ImgPipeline,\n",
    "    StableDiffusionXLImg2ImgPipeline,\n",
    "    StableDiffusionControlNetPipeline,\n",
    "    StableDiffusionXLControlNetImg2ImgPipeline,\n",
    "    StableDiffusionXLControlNetPipeline,\n",
    "    ControlNetModel,\n",
    "    UniPCMultistepScheduler,\n",
    "    AutoencoderKL,\n",
    ")\n",
    "from diffusers.blip.models.blip import blip_decoder\n",
    "\n",
    "\n",
    "def load_demo_image(image_path, image_size, device):\n",
    "    raw_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    w, h = raw_image.size\n",
    "    # display(raw_image.resize((w//5,h//5)))\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        ]\n",
    "    )\n",
    "    image = transform(raw_image).unsqueeze(0).to(device)\n",
    "    return image\n",
    "\n",
    "\n",
    "def generate_pipeline(model_id, device, mode=\"SD\", control_model_id=None):\n",
    "    if mode == \"SD\":\n",
    "        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "            model_id, torch_dtype=torch.float16, use_safetensors=True\n",
    "        ).to(device)\n",
    "    elif mode == \"SDXL\":\n",
    "        pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "            model_id, torch_dtype=torch.float16, use_safetensors=True\n",
    "        ).to(device)\n",
    "    elif mode == \"Control_SD\":\n",
    "        if control_model_id is None:\n",
    "            controlnet = ControlNetModel.from_pretrained(\n",
    "                \"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16, use_safetensors=True\n",
    "            ).to(device)\n",
    "        else:\n",
    "            controlnet = ControlNetModel.from_pretrained(\n",
    "                control_model_id, torch_dtype=torch.float16, use_safetensors=True\n",
    "            ).to(device)\n",
    "\n",
    "        pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "            model_id, controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True\n",
    "        ).to(device)\n",
    "        pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "        # pipe.enable_model_cpu_offload()\n",
    "\n",
    "    elif mode == \"Control_SDXL\":\n",
    "        if control_model_id is None:\n",
    "            controlnet = ControlNetModel.from_pretrained(\n",
    "                \"diffusers/controlnet-canny-sdxl-1.0\", torch_dtype=torch.float16, use_safetensors=True\n",
    "            ).to(device)\n",
    "        else:\n",
    "            controlnet = ControlNetModel.from_pretrained(\n",
    "                control_model_id, torch_dtype=torch.float16, use_safetensors=True\n",
    "            ).to(device)\n",
    "\n",
    "        vae = AutoencoderKL.from_pretrained(\n",
    "            \"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16, use_safetensoƒrs=True\n",
    "        ).to(device)\n",
    "        pipe = StableDiffusionXLControlNetImg2ImgPipeline.from_pretrained(\n",
    "            model_id, controlnet=controlnet, vae=vae, torch_dtype=torch.float16, use_safetensors=True\n",
    "        ).to(device)\n",
    "        pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "        # pipe.enable_model_cpu_offload()\n",
    "    else:\n",
    "        raise ValueError('model must be in [\"SD\", \"SDXL\", \"ControlNet_SD\", \"ControlNet_SDXL\"]')\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def inference_sim2real(pipe, prompt, negative_prompt, image, control_image, mode):\n",
    "    if mode.startswith(\"Control\"):\n",
    "        image = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            image=image,\n",
    "            guidance_scale=8.0,\n",
    "            num_inference_steps=50,\n",
    "            generator=torch.manual_seed(123),\n",
    "        ).images[0]\n",
    "    else:\n",
    "        image = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            image=image,\n",
    "            strength=0.5,\n",
    "            guidance_scale=8.0,\n",
    "            num_inference_steps=50,\n",
    "            generator=torch.manual_seed(123),\n",
    "        ).images[0]\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "clip_device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "clip_image_size = 512\n",
    "clip_model_path = \"/data/noah/ckpt/pretrain_ckpt/BLIP/model_large_caption.pth\"\n",
    "clip_model = blip_decoder(pretrained=clip_model_path, image_size=clip_image_size, vit=\"large\")\n",
    "clip_model.eval()\n",
    "clip_model = clip_model.to(clip_device)\n",
    "\n",
    "sim2real_device = \"cuda:3\"\n",
    "sim2real_mode = \"Control_SDXL\"\n",
    "sim2real_model_id = \"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/sdxl\"\n",
    "sim2real_control_model_id = \"/data/noah/ckpt/finetuning/Control_SDXL_AD/controlnet_25k\"\n",
    "sim2real_pipe = generate_pipeline(\n",
    "    sim2real_model_id, sim2real_device, mode=sim2real_mode, control_model_id=sim2real_control_model_id\n",
    ")\n",
    "\n",
    "image_dir = \"/data/noah/inference/sim2real/_input\"\n",
    "out_dir = \"/data/noah/inference/sim2real/output\"\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "prompt = \"{} ,outdoor, best quality, extremely detailed, clearness, naturalness, film grain, crystal clear, photo with color, actuality\"\n",
    "negative_prompt = \"cartoon, anime, painting, disfigured, immature, blur, picture, 3D, render, semi-realistic, drawing, poorly drawn, bad anatomy, wrong anatomy, gray scale, worst quality, low quality, sketch\"\n",
    "\n",
    "\n",
    "for _ in os.listdir(image_dir):\n",
    "    image_path = os.path.join(image_dir, _)\n",
    "    clip_image = load_demo_image(image_path=image_dir, image_size=clip_image_size, device=device)\n",
    "    sim2real_image = Image.open(image_path).convert(\"RGB\")\n",
    "    sim2real_image = sim2real_image.resize((1024, 1024))\n",
    "    sim2real_canny_image = cv2.Canny(np.array(sim2real_image), low_threshold, high_threshold)\n",
    "    sim2real_canny_image = sim2real_canny_image[:, :, None]\n",
    "    sim2real_canny_image = np.concatenate([sim2real_canny_image, sim2real_canny_image, sim2real_canny_image], axis=2)\n",
    "    sim2real_canny_image = Image.fromarray(sim2real_canny_image)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        caption = clip_model.generate(clip_image, sample=False, num_beams=3, max_length=40, min_length=5)[0]\n",
    "        # caption = clip_model.generate(image, sample=True, top_p=0.9, max_length=20, min_length=5)[0]\n",
    "\n",
    "        sim2real_result_image = inference_sim2real(\n",
    "            sim2real_pipe, prompt.format(caption), negative_prompt, sim2real_canny_image, sim2real_mode\n",
    "        )\n",
    "\n",
    "    sim2real_result_image.save(os.path.join(out_dir, _))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
