{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"GroundingDINO\"))\n",
    "\n",
    "import argparse\n",
    "import copy\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "# Grounding DINO\n",
    "import groundingdino.datasets.transforms as T\n",
    "from groundingdino.models import build_model\n",
    "from groundingdino.util import box_ops\n",
    "from groundingdino.util.slconfig import SLConfig\n",
    "from groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "from groundingdino.util.inference import annotate, load_image, predict\n",
    "\n",
    "import supervision as sv\n",
    "\n",
    "# segment anything\n",
    "from segment_anything import build_sam, SamPredictor\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# diffusers\n",
    "import PIL\n",
    "import requests\n",
    "import torch\n",
    "from io import BytesIO\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "device = torch.device(\"cuda:2\")\n",
    "\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "\n",
    "def load_model(model_config_path, model_checkpoint_path, device):\n",
    "    args = SLConfig.fromfile(model_config_path)\n",
    "    args.device = device\n",
    "    model = build_model(args)\n",
    "    checkpoint = torch.load(model_checkpoint_path, map_location=\"cpu\")\n",
    "    load_res = model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n",
    "    print(load_res)\n",
    "    _ = model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "base_path = \"/data/noah/inference/sky_replacer\"\n",
    "input_name = \"1652255888589_FR-View-CMR-Wide.png\"\n",
    "input_path = os.path.join(base_path, \"input/{}\".format(input_name))\n",
    "input_ref_path = os.path.join(base_path, \"reference\")\n",
    "\n",
    "output_path = os.path.join(base_path, \"output\")\n",
    "output_mask_path = os.path.join(base_path, \"output_mask\")\n",
    "output_background_path = os.path.join(base_path, \"output_background\")\n",
    "output_inpainting = os.path.join(base_path, \"output_inpainting\")\n",
    "\n",
    "filenmae = \"/data/noah/ckpt/pretrain_ckpt/Grounding_DINO/groundingdino_swinb_cogcoor.pth\"\n",
    "config_filename = \"/workspace/Grounded-Segment-Anything/GroundingDINO/groundingdino/config/GroundingDINO_SwinB.py\"\n",
    "\n",
    "model = load_model(config_filename, filenmae, device=device)\n",
    "\n",
    "sam_checkpoint = \"/data/noah/ckpt/pretrain_ckpt/SAM/sam_vit_h_4b8939.pth\"\n",
    "sam_predictor = SamPredictor(build_sam(checkpoint=sam_checkpoint).to(device))\n",
    "\n",
    "# load base and mask image\n",
    "init_image_source, init_image = load_image(input_path)\n",
    "\n",
    "\n",
    "# detect object using grounding DINO\n",
    "def detect(image, text_prompt, model, box_threshold=0.3, text_threshold=0.25):\n",
    "    boxes, logits, phrases = predict(\n",
    "        model=model, image=image, caption=text_prompt, box_threshold=box_threshold, text_threshold=text_threshold\n",
    "    )\n",
    "\n",
    "    annotated_frame = annotate(image_source=init_image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "    annotated_frame = annotated_frame[..., ::-1]  # BGR to RGB\n",
    "    return annotated_frame, boxes\n",
    "\n",
    "\n",
    "annotated_frame, detected_boxes = detect(init_image, text_prompt=\"sky\", model=model)\n",
    "\n",
    "\n",
    "def segment(image, sam_model, boxes):\n",
    "    sam_model.set_image(image)\n",
    "    H, W, _ = image.shape\n",
    "    boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
    "\n",
    "    transformed_boxes = sam_model.transform.apply_boxes_torch(boxes_xyxy.to(device), image.shape[:2])\n",
    "    masks, _, _ = sam_model.predict_torch(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        boxes=transformed_boxes,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    return masks.cpu()\n",
    "\n",
    "\n",
    "def draw_mask(mask, image, random_color=True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.8])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "\n",
    "    annotated_frame_pil = Image.fromarray(image).convert(\"RGBA\")\n",
    "    mask_image_pil = Image.fromarray((mask_image.cpu().numpy() * 255).astype(np.uint8)).convert(\"RGBA\")\n",
    "\n",
    "    return np.array(Image.alpha_composite(annotated_frame_pil, mask_image_pil))\n",
    "\n",
    "\n",
    "segmented_frame_masks = segment(init_image_source, sam_predictor, boxes=detected_boxes)\n",
    "annotated_frame_with_mask = draw_mask(segmented_frame_masks[0][0], annotated_frame)\n",
    "\n",
    "mask = segmented_frame_masks[0][0].cpu().numpy()\n",
    "inverted_mask = ((1 - mask) * 255).astype(np.uint8)\n",
    "\n",
    "image_source_pil = Image.fromarray(init_image_source)\n",
    "image_mask_pil = Image.fromarray(mask)\n",
    "image_mask_pil.save(os.path.join(output_mask_path, input_name))\n",
    "\n",
    "display(image_mask_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "\n",
    "\n",
    "def partial_histogram_matching(source_image, target_image, alpha=0.15):\n",
    "    corrected_channels = []\n",
    "\n",
    "    for source_channel, target_channel in zip(cv2.split(source_image), cv2.split(target_image)):\n",
    "        # 히스토그램 계산\n",
    "        source_hist = cv2.calcHist([source_channel], [0], None, [256], [0, 256])\n",
    "        target_hist = cv2.calcHist([target_channel], [0], None, [256], [0, 256])\n",
    "\n",
    "        # 전체 픽셀수로 히스토그램 정규화\n",
    "        source_hist /= source_channel.size\n",
    "        target_hist /= target_channel.size\n",
    "\n",
    "        # 누적 분포 계산\n",
    "        source_cdf = np.cumsum(source_hist)\n",
    "        target_cdf = np.cumsum(target_hist)\n",
    "\n",
    "        # 누적 분포 정규화\n",
    "        source_cdf = source_cdf / source_cdf[-1]\n",
    "        target_cdf = target_cdf / target_cdf[-1]\n",
    "\n",
    "        # souce 이미지의 히스토그램 누적 분포를 target 이미지의 히스토그램 누적 분포로 매칭시키는 룩업 테이블 생성\n",
    "        lut = np.interp(source_cdf, target_cdf, range(256))\n",
    "\n",
    "        # alpha 값을 통한 룩업 테이블 수정\n",
    "        partial_lut = alpha * lut + (1 - alpha) * np.arange(256)\n",
    "\n",
    "        # 룩업 데이블 기반 source 이미지 보정\n",
    "        corrected_channel = cv2.LUT(source_channel, partial_lut.astype(\"uint8\"))\n",
    "        corrected_channels.append(corrected_channel)\n",
    "\n",
    "    corrected_image = cv2.merge(corrected_channels)\n",
    "\n",
    "    return corrected_image\n",
    "\n",
    "\n",
    "corrected_images = []\n",
    "ref_images = []\n",
    "ref_names = os.listdir(input_ref_path)\n",
    "\n",
    "for r_name in ref_names:\n",
    "    ref_image = load_image(os.path.join(input_ref_path, r_name))\n",
    "    ref_image = ref_image.resize((image_source_pil.width, image_source_pil.height))\n",
    "    ref_images.append(ref_image)\n",
    "\n",
    "    corrected_imageA = partial_histogram_matching(init_image_source, np.array(ref_image))\n",
    "    corrected_imageA = Image.fromarray(corrected_imageA)\n",
    "    corrected_imageA.save(os.path.join(output_background_path, \"{}_{}\".format(input_name, r_name)))\n",
    "    corrected_images.append(corrected_imageA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoPipelineForInpainting\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def color_clahe_equalization(image, clip_limit=1.0, tile_grid_size=(4, 4)):\n",
    "    # Convert the image from BGR to LAB color space\n",
    "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # Split the LAB image into L, A, and B channels\n",
    "    l_channel, a_channel, b_channel = cv2.split(lab_image)\n",
    "\n",
    "    # Apply CLAHE to the L channel\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "    cl_channel = clahe.apply(l_channel)\n",
    "\n",
    "    # Merge the CLAHE-enhanced L channel with the original A and B channels\n",
    "    equalized_lab_image = cv2.merge([cl_channel, a_channel, b_channel])\n",
    "\n",
    "    # Convert the image back to BGR color space\n",
    "    equalized_image = cv2.cvtColor(equalized_lab_image, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    return equalized_image\n",
    "\n",
    "\n",
    "device = \"cuda:2\"\n",
    "resolution = 1024\n",
    "# init_image = image_source_pil.resize((resolution, resolution))\n",
    "mask_image = image_mask_pil.resize((resolution, resolution))\n",
    "\n",
    "pipeline = AutoPipelineForInpainting.from_pretrained(\n",
    "    \"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/inpaint\", torch_dtype=torch.float16\n",
    ")\n",
    "pipeline.load_ip_adapter(\n",
    "    \"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/ip-adapter\",\n",
    "    subfolder=\"models\",\n",
    "    weight_name=\"ip-adapter_sd15.bin\",\n",
    ")\n",
    "pipeline = pipeline.to(device)\n",
    "prompt = \"\"\n",
    "\n",
    "for corrected_image, ref_image, ref_name in zip(corrected_images, ref_images, ref_names):\n",
    "    corrected_image = corrected_image.resize((resolution, resolution))\n",
    "    ref_image = ref_image.resize((resolution, resolution))\n",
    "\n",
    "    image = pipeline(\n",
    "        prompt=prompt,\n",
    "        image=corrected_image,\n",
    "        mask_image=mask_image,\n",
    "        strength=0.8,\n",
    "        guidance_scale=7.5,\n",
    "        ip_adapter_image=ref_image,\n",
    "        height=1024,\n",
    "        width=1024,\n",
    "    ).images[0]\n",
    "\n",
    "    image = image.resize((image_mask_pil.width, image_mask_pil.height))\n",
    "    image.save(os.path.join(output_inpainting, \"{}_{}\".format(input_name, ref_name)))\n",
    "    image = color_clahe_equalization(np.array(image))\n",
    "    image = Image.fromarray(image)\n",
    "    image.save(os.path.join(output_path, \"{}_{}\".format(input_name, ref_name)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
