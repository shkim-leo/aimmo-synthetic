{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "base_image_path = \"/data/noah/inference/magna_rider_premask/_images\"\n",
    "image_path = \"/data/noah/inference/magna_rider_premask/images\"\n",
    "mask_path = \"/data/noah/inference/magna_rider_premask/masks\"\n",
    "\n",
    "for name in os.listdir(mask_path):\n",
    "    src_path = os.path.join(base_image_path, name)\n",
    "    dst_path = os.path.join(image_path, name)\n",
    "\n",
    "    shutil.copy(src_path, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make grid\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def make_grid(images, rows, cols):\n",
    "    w, h = images[0].size\n",
    "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
    "    for i, image in enumerate(images):\n",
    "        grid.paste(image, box=(i % cols * w, i // cols * h))\n",
    "    return grid\n",
    "\n",
    "\n",
    "image_paths = [\n",
    "    \"/data/noah/inference/magna_controlnet_inpainting_shadow/results/2022-11-04-14-28-47_006173_rear_rectilinear_rgb.jpg\",\n",
    "    \"/data/noah/inference/magna_controlnet_inpainting_shadow/results_shadow/2022-11-04-14-28-47_006173_rear_rectilinear_rgb.jpg\",\n",
    "    \"/data/noah/inference/magna_controlnet_inpainting_shadow/results/2022-10-14-12-36-58_004680_left_rectilinear_rgb.jpg\",\n",
    "    \"/data/noah/inference/magna_controlnet_inpainting_shadow/results_shadow/2022-10-14-12-36-58_004680_left_rectilinear_rgb.jpg\"    \n",
    "]\n",
    "images = []\n",
    "for idx, image_path in enumerate(image_paths):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    print(image.size)\n",
    "\n",
    "    if idx==0:\n",
    "        image=image.resize((image.width, image.height))\n",
    "        images.append(image)\n",
    "    else:\n",
    "\n",
    "        images.append(image)\n",
    "\n",
    "grid = make_grid(images, 2, 2)\n",
    "\n",
    "display(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크기가 큰 AD Dataset 이미지 선별\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import shutil\n",
    "annotation_path = '/data/noah/dataset/ad_human/anno'\n",
    "dst_path = '/data/noah/inference/face'\n",
    "\n",
    "for annotation_name in os.listdir(annotation_path):\n",
    "    with open(os.path.join(annotation_path, annotation_name), 'r') as f:\n",
    "        annotation = json.load(f)\n",
    "\n",
    "    for ann in annotation['annotations']:\n",
    "        mask = np.zeros((annotation['metadata']['height'], annotation['metadata']['width']))\n",
    "\n",
    "        if ann['label'] == 'pedestrian':\n",
    "            points = np.array(ann['points'], dtype=np.int32)\n",
    "            mask = cv2.fillPoly(mask, [points], 255)            \n",
    "            area = np.sum(mask==255)\n",
    "            if area<60000 and area>50000:\n",
    "                print(os.path.join(annotation['parent_path'], annotation['filename']))\n",
    "                shutil.copy(os.path.join(annotation['parent_path'], annotation['filename']), os.path.join(dst_path, annotation['filename']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "from diffusers import StableDiffusionInpaintPipeline, DDIMScheduler\n",
    "from controlnet_aux.processor import MidasDetector\n",
    "import sys\n",
    "\n",
    "# Grounding DINO\n",
    "import groundingdino.datasets.transforms as T\n",
    "from groundingdino.models import build_model\n",
    "from groundingdino.util import box_ops\n",
    "from groundingdino.util.slconfig import SLConfig\n",
    "from groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "from groundingdino.util.inference import annotate, load_image, predict\n",
    "\n",
    "import supervision as sv\n",
    "\n",
    "# segment anything\n",
    "from segment_anything import build_sam, SamPredictor\n",
    "\n",
    "def load_model(model_config_path, model_checkpoint_path, device):\n",
    "    args = SLConfig.fromfile(model_config_path)\n",
    "    args.device = device\n",
    "    model = build_model(args)\n",
    "    checkpoint = torch.load(model_checkpoint_path, map_location=\"cpu\")\n",
    "    load_res = model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n",
    "    print(load_res)\n",
    "    _ = model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# detect object using grounding DINO\n",
    "def detect(image, image_source, text_prompt, model, box_threshold=0.35, text_threshold=0.35):\n",
    "    boxes, logits, phrases = predict(\n",
    "        model=model, image=image, caption=text_prompt, box_threshold=box_threshold, text_threshold=text_threshold\n",
    "    )\n",
    "\n",
    "    annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "    annotated_frame = annotated_frame[..., ::-1]  # BGR to RGB\n",
    "    return annotated_frame, boxes\n",
    "\n",
    "\n",
    "def segment(image, sam_model, boxes):\n",
    "    sam_model.set_image(image)\n",
    "    H, W, _ = image.shape\n",
    "    boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
    "\n",
    "    transformed_boxes = sam_model.transform.apply_boxes_torch(boxes_xyxy.to(device), image.shape[:2])\n",
    "    masks, _, _ = sam_model.predict_torch(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        boxes=transformed_boxes,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    return masks.cpu()\n",
    "\n",
    "def get_mask(image, dino, sam):\n",
    "    result_mask = np.zeros((image.shape[0], image.shape[1]))\n",
    "\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.RandomResize([800], max_size=1333),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    image_torch, _ = transform(Image.fromarray(image).convert(\"RGB\"), None)\n",
    "    annotated_frame, detected_boxes = detect(\n",
    "        image_torch, image, text_prompt=\"face\", model=dino\n",
    "    )\n",
    "    if len(detected_boxes) == 0:\n",
    "        return None\n",
    "\n",
    "    seg_result = segment(image, sam, boxes=detected_boxes)\n",
    "    result_masks = []\n",
    "    for seg_map in seg_result:\n",
    "        mask = seg_map[0].cpu().numpy().astype(np.uint8) * 255\n",
    "        result_masks.append(mask)\n",
    "\n",
    "    return result_masks\n",
    "\n",
    "def closest_multiple_of_8(number):\n",
    "    closest_multiple = (number // 8) * 8  # 가장 가까운 8의 배수\n",
    "    return closest_multiple\n",
    "\n",
    "base_path = \"/data/noah/inference/face\"\n",
    "out_path = \"/data/noah/inference/face_out\"\n",
    "os.makedirs(out_path,exist_ok=True)\n",
    "device = \"cuda:2\"\n",
    "grounding_dino_ckpt_path = \"/data/noah/ckpt/pretrain_ckpt/Grounding_DINO/groundingdino_swinb_cogcoor.pth\"\n",
    "grounding_dino_config_path = (\n",
    "    \"/workspace/Grounded-Segment-Anything/GroundingDINO/groundingdino/config/GroundingDINO_SwinB.py\"\n",
    ")\n",
    "grounding_dino = load_model(grounding_dino_config_path, grounding_dino_ckpt_path, device=device)\n",
    "sam_ckpt_path = \"/data/noah/ckpt/pretrain_ckpt/SAM/sam_vit_h_4b8939.pth\"\n",
    "sam_predictor = SamPredictor(build_sam(checkpoint=sam_ckpt_path).to(device))\n",
    "\n",
    "pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    \"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/rv_inpaint_5.1\", torch_dtype=torch.float16\n",
    ").to(device)\n",
    "pipe.load_lora_weights(\n",
    "    \"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/lora_detail\", weight_name=\"add_detail.safetensors\"\n",
    ")\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_freeu(s1=0.9, s2=0.2, b1=1.2, b2=1.4)\n",
    "prompt = \"man, best quality, extremely detailed, clearness, crystal clear, <lora:add-detail:-1>\"\n",
    "negative_prompt = \"cartoon, anime, painting, disfigured, immature, blur, picture, semi-realistic, gray scale, worst quality, low quality, out of frame, jpeg artifacts, ugly, poorly drawn eyes\"\n",
    "\n",
    "for idx, _ in enumerate(os.listdir(base_path)):\n",
    "    image_path = os.path.join(base_path, _)\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    height, width, channel = image.shape\n",
    "    height, width = closest_multiple_of_8(height), closest_multiple_of_8(width)\n",
    "    image = cv2.resize(image, (width, height))\n",
    "\n",
    "    masks = get_mask(image, grounding_dino, sam_predictor)\n",
    "\n",
    "    if masks is None:\n",
    "        print('{} passed!'.format(image_path))\n",
    "        continue\n",
    "    \n",
    "    overlay_image = np.copy(image)\n",
    "    sum_mask = np.zeros((height, width))\n",
    "    for mask in masks:\n",
    "        sum_mask += mask\n",
    "        k = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, k, iterations=5)\n",
    "        spot = np.argwhere(mask==255).tolist()\n",
    "        blurred_mask = pipe.mask_processor.blur(Image.fromarray(mask).convert('L'), blur_factor=10)\n",
    "        result_image = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            image=Image.fromarray(image),\n",
    "            mask_image=blurred_mask,\n",
    "            height=image.shape[0],\n",
    "            width=image.shape[1],\n",
    "            num_inference_steps=30,\n",
    "            guidance_scale=7.5,\n",
    "            # generator=generator\n",
    "        ).images[0]\n",
    "        \n",
    "        result_image = np.array(result_image)\n",
    "\n",
    "        for s in spot:\n",
    "            overlay_image[s[0], s[1], :] = result_image[s[0], s[1], :]\n",
    "    \n",
    "    overlay_image = Image.fromarray(overlay_image)\n",
    "    sum_mask = Image.fromarray(sum_mask).convert('L')\n",
    "    print(_)\n",
    "    overlay_image.save(os.path.join(out_path, '{}_out.png'.format(_)))\n",
    "    sum_mask.save(os.path.join(out_path, '{}_mask.png'.format(_)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
