{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def mask_to_polygon(mask):\n",
    "    # 윤곽선 찾기\n",
    "    contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # 윤곽선을 다각형으로 변환\n",
    "    polygons = []\n",
    "    for contour in contours:\n",
    "        contour = contour.squeeze(axis=1)  # 차원 축소\n",
    "        polygon = contour[:, [0, 1]].tolist()  # (y, x) 순서로 변환하여 리스트로 저장\n",
    "        polygons.append(polygon)\n",
    "\n",
    "    return polygons\n",
    "\n",
    "\n",
    "def polygon_to_mask(mask, polygons, color=255):\n",
    "    polygons = np.array(polygons, dtype=np.int32)\n",
    "    state = False\n",
    "\n",
    "    try:\n",
    "        mask = cv2.fillPoly(mask.astype(\"uint8\"), [polygons], color)\n",
    "        state = True\n",
    "    except:\n",
    "        print(\"mask passed!\")\n",
    "\n",
    "    return mask, state\n",
    "\n",
    "\n",
    "def make_dirs(paths):\n",
    "    for path in paths:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def crop_from_mask(image, mask):\n",
    "    # Find contours in the mask\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # If no contours found, return original image\n",
    "    if not contours:\n",
    "        return image\n",
    "\n",
    "    # Find the bounding box of the largest contour\n",
    "    x, y, w, h = cv2.boundingRect(contours[0])\n",
    "\n",
    "    # Crop the image using the bounding box\n",
    "    cropped_image = image[y : y + h, x : x + w]\n",
    "    cropped_mask = mask[y : y + h, x : x + w]\n",
    "\n",
    "    return cropped_image, cropped_mask\n",
    "\n",
    "\n",
    "annotation_path = \"/data/noah/dataset/coco_rider/anno\"\n",
    "out_base_path = \"/data/noah/dataset/coco_rider/magna_rider_premask\"\n",
    "out_mask_path = os.path.join(out_base_path, \"masks\")\n",
    "out_image_path = os.path.join(out_base_path, \"images\")\n",
    "\n",
    "make_dirs([out_base_path, out_mask_path, out_image_path])\n",
    "cnt = 0\n",
    "\n",
    "for name in tqdm(os.listdir(annotation_path)):\n",
    "    ann_path = os.path.join(annotation_path, name)\n",
    "\n",
    "    with open(ann_path, \"r\") as f:\n",
    "        ann = json.load(f)\n",
    "\n",
    "    height, width = ann[\"metadata\"][\"height\"], ann[\"metadata\"][\"width\"]\n",
    "    mask = np.zeros((height, width))\n",
    "\n",
    "    for _ann in ann[\"annotations\"]:\n",
    "        point = np.array(_ann[\"points\"], dtype=np.int32)\n",
    "        try:\n",
    "            mask = cv2.fillPoly(mask, [point], color=255)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    polygons = mask_to_polygon(mask)\n",
    "    annotations = []\n",
    "    image = Image.open(os.path.join(ann[\"parent_path\"], ann[\"filename\"]))\n",
    "\n",
    "    for polygon in polygons:\n",
    "        cnt += 1\n",
    "        mask = np.zeros((height, width))\n",
    "        mask, state = polygon_to_mask(mask, polygon)\n",
    "\n",
    "        if state:\n",
    "            crop_image, crop_mask = crop_from_mask(np.array(image).astype(\"uint8\"), mask.astype(\"uint8\"))\n",
    "            crop_image = Image.fromarray(crop_image)\n",
    "            crop_mask = Image.fromarray(crop_mask).convert(\"L\")\n",
    "\n",
    "            crop_image.save(os.path.join(out_image_path, \"{}.png\".format(cnt)))\n",
    "            crop_mask.save(os.path.join(out_mask_path, \"{}.png\".format(cnt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "from diffusers import StableDiffusionInpaintPipeline, ControlNetModel, DDIMScheduler\n",
    "from controlnet_aux.processor import MidasDetector\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../harmonization\")\n",
    "from harmonization import Harmonization\n",
    "from gtgen.bpr import GtGenBPRInference\n",
    "\n",
    "\n",
    "def make_inputs(image, annotation, target_indexs, harmonizer):\n",
    "    height, width = annotation[\"metadata\"][\"height\"], annotation[\"metadata\"][\"width\"]\n",
    "    mask = np.zeros((height, width))\n",
    "    spot = None\n",
    "    k = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "\n",
    "    while True:\n",
    "        target_index = random.choice(target_indexs)\n",
    "        rb_spot = random_coordinate(annotation[\"annotations\"], target_index, height, width)  # height, width 순\n",
    "\n",
    "        if rb_spot is None:\n",
    "            print(\"{} can not generate right bottom spot\".format(annotation[\"filename\"]))\n",
    "            return None\n",
    "\n",
    "        # rb_spot x값을 기준으로 height 선정 및 target_height 산출\n",
    "        target_height = random.randint(500, 500)\n",
    "        mask_name = random.choice(mask_lists)\n",
    "\n",
    "        image_pth = os.path.join(image_path, mask_name)\n",
    "        mask_pth = os.path.join(mask_path, mask_name)\n",
    "\n",
    "        paste_image = Image.open(image_pth)\n",
    "        paste_mask = Image.open(mask_pth)\n",
    "\n",
    "        ratio = float(target_height) / paste_mask.height\n",
    "        paste_mask = paste_mask.resize((int(paste_mask.width * ratio), int(paste_mask.height * ratio)))\n",
    "        paste_image = paste_image.resize((int(paste_image.width * ratio), int(paste_image.height * ratio)))\n",
    "\n",
    "        paste_mask = np.array(paste_mask).astype(\"uint8\")\n",
    "        paste_image = np.array(paste_image).astype(\"uint8\")\n",
    "\n",
    "        if np.sum(paste_mask) == 0:\n",
    "            continue\n",
    "\n",
    "        paste_mask = cv2.morphologyEx(paste_mask, cv2.MORPH_OPEN, k, iterations=5)\n",
    "        paste_mask = np.where(paste_mask > 127, 255, 0).astype(\"uint8\")\n",
    "        sum_mask = add_mask(mask, paste_mask, rb_spot[1], rb_spot[0])\n",
    "\n",
    "        if sum_mask is None:\n",
    "            continue\n",
    "\n",
    "        spot = np.argwhere(sum_mask == 255)\n",
    "        print(mask_name)\n",
    "        print(image.shape)\n",
    "        print(paste_image.shape)\n",
    "        sum_image = add_image(image, paste_image, paste_mask, rb_spot[1], rb_spot[0])\n",
    "        break\n",
    "\n",
    "    sum_image = harmonizer.harmonize(sum_image, sum_mask)\n",
    "    sum_image = Image.fromarray(sum_image.astype(\"uint8\")).convert(\"RGB\")\n",
    "    sum_mask = Image.fromarray(sum_mask.astype(\"uint8\")).convert(\"L\")\n",
    "\n",
    "    output = {\"image\": sum_image, \"mask\": sum_mask, \"spot\": spot}\n",
    "    return output\n",
    "\n",
    "\n",
    "def make_grid(images, rows, cols):\n",
    "    w, h = images[0].size\n",
    "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
    "    for i, image in enumerate(images):\n",
    "        grid.paste(image, box=(i % cols * w, i // cols * h))\n",
    "    return grid\n",
    "\n",
    "\n",
    "def find_outer_contour_coordinates(mask):\n",
    "    # OpenCV의 findContours 함수를 사용하여 이진 이미지의 외곽선을 찾습니다.\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # 외곽선을 이루는 점들의 좌표를 반환합니다.\n",
    "    outer_contour_coords = [[], []]\n",
    "    for contour in contours:\n",
    "        for point in contour:\n",
    "            x, y = point[0]\n",
    "            outer_contour_coords[0].append(y)\n",
    "            outer_contour_coords[1].append(x)\n",
    "\n",
    "    return outer_contour_coords\n",
    "\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    return np.sqrt((point2[0] - point1[0]) ** 2 + (point2[1] - point1[1]) ** 2)\n",
    "\n",
    "\n",
    "def random_coordinate(annotation, target_index, height, width):\n",
    "    mask = np.zeros((height, width))\n",
    "    mask, state = polygon_to_mask(mask, annotation[target_index][\"points\"], color=255)\n",
    "\n",
    "    if not state:\n",
    "        return None\n",
    "\n",
    "    for idx, ann in enumerate(annotation):\n",
    "        if idx == target_index:\n",
    "            continue\n",
    "\n",
    "        mask, state = polygon_to_mask(mask, ann[\"points\"], color=0)\n",
    "\n",
    "    target_spots = np.argwhere(mask == 255).tolist()\n",
    "\n",
    "    if len(target_spots) == 0:\n",
    "        return None\n",
    "\n",
    "    coordinates = find_outer_contour_coordinates(mask)\n",
    "    threshold = 500\n",
    "\n",
    "    # 랜덤으로 좌표 선택\n",
    "    while True:\n",
    "        target_spot = random.choice(target_spots)  # height,width 순\n",
    "        distances = [\n",
    "            euclidean_distance((coord[0], coord[1]), target_spot) for coord in zip(coordinates[0], coordinates[1])\n",
    "        ]\n",
    "        min_distance = int(min(distances))\n",
    "\n",
    "        if min_distance >= threshold:\n",
    "            return target_spot\n",
    "        else:\n",
    "            threshold = threshold // 2\n",
    "\n",
    "\n",
    "def add_mask(mask, new_mask, right, bottom):\n",
    "    mask_cp = mask.copy()\n",
    "\n",
    "    # 새로운 마스크를 더할 위치 계산\n",
    "    left = right - new_mask.shape[1]\n",
    "    top = bottom - new_mask.shape[0]\n",
    "\n",
    "    # 마스크 영역에 새로운 마스크 더하기\n",
    "    if left < 0 or top < 0:\n",
    "        return None\n",
    "\n",
    "    mask_cp[top:bottom, left:right] += new_mask\n",
    "\n",
    "    return mask_cp\n",
    "\n",
    "\n",
    "def add_image(image, new_image, mask, right, bottom):\n",
    "    image_cp = image.copy()\n",
    "\n",
    "    # 새로운 마스크를 더할 위치 계산\n",
    "    left = right - new_image.shape[1]\n",
    "    top = bottom - new_image.shape[0]\n",
    "\n",
    "    # 마스크 영역에 새로운 마스크 더하기\n",
    "    if left < 0 or top < 0:\n",
    "        return None\n",
    "\n",
    "    for h in range(top, bottom):\n",
    "        for w in range(left, right):\n",
    "            if mask[h - top, w - left]:\n",
    "                image_cp[h, w, :] = new_image[h - top, w - left, :]\n",
    "\n",
    "    return image_cp\n",
    "\n",
    "\n",
    "def make_dirs(paths):\n",
    "    for path in paths:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def make_result(image, mask):\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    image_with_mask_contours = np.copy(image)\n",
    "    cv2.drawContours(image_with_mask_contours, contours, -1, (0, 0, 255), 2)\n",
    "    return image_with_mask_contours\n",
    "\n",
    "\n",
    "def mask_refinement(image, ann, bpr_inference):\n",
    "    seg_result = bpr_inference.inference(\n",
    "        img=image,\n",
    "        seg=ann,\n",
    "        img_scale=(256, 256),\n",
    "        img_ratios=[1.0, 2.0],\n",
    "        nms_iou_threshold=0.5,\n",
    "        point_density=0.25,\n",
    "        patch_size=[32, 64, 96],\n",
    "        padding=0,\n",
    "    )\n",
    "\n",
    "    height, width = image.shape[0], image.shape[1]\n",
    "    result_map = np.zeros((height, width))\n",
    "\n",
    "    for sr in seg_result[\"annotations\"]:\n",
    "        result_map, state = polygon_to_mask(result_map, sr[\"points\"], 255)\n",
    "\n",
    "    return result_map.astype(\"uint8\")\n",
    "\n",
    "\n",
    "def mask_to_polygon(mask):\n",
    "    # 윤곽선 찾기\n",
    "    contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # 윤곽선을 다각형으로 변환\n",
    "    polygons = []\n",
    "    for contour in contours:\n",
    "        contour = contour.squeeze(axis=1)  # 차원 축소\n",
    "        polygon = contour[:, [0, 1]].tolist()  # (y, x) 순서로 변환하여 리스트로 저장\n",
    "        polygons.append(polygon)\n",
    "\n",
    "    return polygons\n",
    "\n",
    "\n",
    "def polygon_to_mask(mask, polygons, color=255):\n",
    "    polygons = np.array(polygons, dtype=np.int32)\n",
    "    state = False\n",
    "\n",
    "    try:\n",
    "        mask = cv2.fillPoly(mask.astype(\"uint8\"), [polygons], color)\n",
    "        state = True\n",
    "    except:\n",
    "        print(\"mask passed!\")\n",
    "\n",
    "    return mask, state\n",
    "\n",
    "\n",
    "def modify_annotation(annotations, polygons, height, width):\n",
    "    # draw generated mask\n",
    "    generated_mask = np.zeros((height, width))\n",
    "    generated_annotations = []\n",
    "    original_annotations = []\n",
    "\n",
    "    for polygon in polygons:\n",
    "        generated_mask, state = polygon_to_mask(generated_mask, polygon, 255)\n",
    "\n",
    "        if state:\n",
    "            ann = {\n",
    "                \"id\": \"\",\n",
    "                \"type\": \"poly_seg\",\n",
    "                \"attributes\": {},\n",
    "                \"points\": polygon,\n",
    "                \"label\": \"rider\",\n",
    "            }\n",
    "            generated_annotations.append(ann)\n",
    "\n",
    "    for annotation in annotations:\n",
    "        # draw original mask\n",
    "        original_mask = np.zeros((height, width))\n",
    "        original_mask, state = polygon_to_mask(original_mask, annotation[\"points\"], 255)\n",
    "\n",
    "        if not state:\n",
    "            continue\n",
    "\n",
    "        # modify original mask\n",
    "        original_mask = np.where((original_mask == 255) & (generated_mask == 255), 0, original_mask)\n",
    "        original_polygons = mask_to_polygon(original_mask)\n",
    "\n",
    "        for polygon in original_polygons:\n",
    "            ann = copy.deepcopy(annotation)\n",
    "            ann[\"points\"] = polygon\n",
    "            original_annotations.append(ann)\n",
    "\n",
    "    original_annotations.extend(generated_annotations)\n",
    "    return original_annotations\n",
    "\n",
    "\n",
    "device = \"cuda:2\"\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "# 사전 정의된 Crop 이미지와 마스크\n",
    "mask_path = \"/data/noah/inference/magna_rider_premask/masks\"\n",
    "image_path = \"/data/noah/inference/magna_rider_premask/images\"\n",
    "mask_lists = os.listdir(mask_path)\n",
    "\n",
    "# 생성할 Annotation 정보\n",
    "base_image_path = \"/data/noah/dataset/magna_traffic_light/pre_images\"\n",
    "target_annotation_path = \"/data/noah/dataset/magna_traffic_light/pre_anno\"\n",
    "target_class_name = \"road\"\n",
    "target_height = None\n",
    "\n",
    "save_base_path = \"/data/noah/inference/magna_rv_inpainting\"\n",
    "save_result_path = os.path.join(save_base_path, \"results\")\n",
    "save_draw_path = os.path.join(save_base_path, \"draw_results\")\n",
    "save_refined_draw_path = os.path.join(save_base_path, \"draw_results_refined\")\n",
    "save_annotation_draw_path = os.path.join(save_base_path, \"annotation_draw\")\n",
    "save_modified_annotation_draw_path = os.path.join(save_base_path, \"modified_annotation_draw\")\n",
    "save_mask_path = os.path.join(save_base_path, \"masks\")\n",
    "make_dirs(\n",
    "    [\n",
    "        save_base_path,\n",
    "        save_result_path,\n",
    "        save_draw_path,\n",
    "        save_refined_draw_path,\n",
    "        save_modified_annotation_draw_path,\n",
    "        save_annotation_draw_path,\n",
    "        save_mask_path,\n",
    "    ]\n",
    ")\n",
    "\n",
    "bpr_inference = GtGenBPRInference(devices=[3], batch_size=48)\n",
    "bpr_model = bpr_inference.load_model(\"/data/noah/ckpt/finetuning/bpr.pth\", img_scale=(256, 256))\n",
    "assert bpr_model is not None, \"model not loaded\"\n",
    "\n",
    "harmonizer = Harmonization(\"/data/noah/ckpt/pretrain_ckpt/duconet/duconet1024.pth\", device=device)\n",
    "\n",
    "pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    \"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/rv_inpaint_5.1\", torch_dtype=torch.float16\n",
    ").to(device)\n",
    "pipe.load_lora_weights(\n",
    "    \"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/lora_detail\", weight_name=\"add_detail.safetensors\"\n",
    ")\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "# generator = torch.Generator(device=device).manual_seed(42)\n",
    "\n",
    "prompt = \"{}, best quality, extremely detailed, clearness, naturalness, film grain, crystal clear, photo with color, actuality\"\n",
    "negative_prompt = \"cartoon, anime, painting, disfigured, immature, blur, picture, 3D, render, semi-realistic, drawing, poorly drawn, bad anatomy, wrong anatomy, gray scale, worst quality, low quality, sketch, text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck\"\n",
    "\n",
    "\n",
    "for ann_idx, ann_name in tqdm(enumerate(os.listdir(target_annotation_path)[:100])):\n",
    "    annotation_path = os.path.join(target_annotation_path, ann_name)\n",
    "\n",
    "    with open(annotation_path, \"r\") as f:\n",
    "        annotation = json.load(f)\n",
    "\n",
    "    target_indexs = []\n",
    "\n",
    "    for idx, ann in enumerate(annotation[\"annotations\"]):\n",
    "        if ann[\"label\"] == target_class_name:\n",
    "            target_indexs.append(idx)\n",
    "            break\n",
    "\n",
    "    if not len(target_indexs):\n",
    "        continue\n",
    "\n",
    "    height, width = annotation[\"metadata\"][\"height\"], annotation[\"metadata\"][\"width\"]\n",
    "    image = Image.open(os.path.join(base_image_path, annotation[\"parent_path\"][1:], annotation[\"filename\"]))\n",
    "    image = np.array(image).astype(\"uint8\")\n",
    "\n",
    "    sum_mask_image = np.zeros((height, width))\n",
    "    sum_result_image = np.copy(image)\n",
    "    sum_draw_image = None\n",
    "\n",
    "    generate_cnt = random.randint(3, 5)\n",
    "    inputs = None\n",
    "    generated_spots = []\n",
    "\n",
    "    for iter_cnt in range(generate_cnt):\n",
    "        inputs = make_inputs(image, annotation, target_indexs, harmonizer)\n",
    "\n",
    "        if inputs is None:\n",
    "            break\n",
    "\n",
    "        if len(generated_spots) and np.any(np.all(np.isin(np.array(generated_spots), inputs[\"spot\"]), axis=1)):\n",
    "            continue\n",
    "        else:\n",
    "            generated_spots.extend(inputs[\"spot\"].tolist())\n",
    "\n",
    "        result_image = pipe(\n",
    "            prompt=prompt.format(\"a rider is on the road\"),\n",
    "            negative_prompt=negative_prompt,\n",
    "            image=inputs[\"image\"],\n",
    "            mask_image=inputs[\"mask\"],\n",
    "            height=inputs[\"image\"].height,\n",
    "            width=inputs[\"image\"].width,\n",
    "            num_inference_steps=25,\n",
    "            guidance_scale=7.5,\n",
    "            # generator=generator\n",
    "        ).images[0]\n",
    "\n",
    "        result_image = harmonizer.harmonize(np.array(result_image), np.array(inputs[\"mask\"]))\n",
    "        result_image = result_image.astype(\"uint8\")\n",
    "\n",
    "        for spot in inputs[\"spot\"]:\n",
    "            sum_result_image[spot[0], spot[1], :] = result_image[spot[0], spot[1], :]\n",
    "\n",
    "        sum_mask_image = sum_mask_image + np.array(inputs[\"mask\"])\n",
    "\n",
    "    if inputs is not None:\n",
    "        # mask refinement\n",
    "        polygons = mask_to_polygon(sum_mask_image)\n",
    "        generated_annotation = copy.deepcopy(annotation)\n",
    "        anns = []\n",
    "\n",
    "        for polygon in polygons:\n",
    "            an = {\n",
    "                \"id\": \"\",\n",
    "                \"type\": \"poly_seg\",\n",
    "                \"attributes\": {},\n",
    "                \"points\": polygon,\n",
    "                \"label\": \"rider\",\n",
    "            }\n",
    "            anns.append(an)\n",
    "\n",
    "        generated_annotation[\"annotations\"] = anns\n",
    "        refined_mask = mask_refinement(sum_result_image, generated_annotation, bpr_inference)\n",
    "\n",
    "        # draw result image with mask\n",
    "        sum_draw_image = make_result(np.copy(sum_result_image), sum_mask_image.astype(\"uint8\"))\n",
    "        sum_draw_refined_image = make_result(np.copy(sum_result_image), refined_mask)\n",
    "\n",
    "        # annotation 수정 작업 #\n",
    "        modified_annotation = modify_annotation(\n",
    "            annotation[\"annotations\"],\n",
    "            polygons,\n",
    "            height,\n",
    "            width,\n",
    "        )\n",
    "\n",
    "        # draw annotation\n",
    "        original_mask = np.zeros((height, width, 3))\n",
    "        modified_mask = np.zeros((height, width, 3))\n",
    "\n",
    "        for ann in annotation[\"annotations\"]:\n",
    "            original_mask, state = polygon_to_mask(\n",
    "                original_mask,\n",
    "                ann[\"points\"],\n",
    "                color=(\n",
    "                    random.randint(0, 255),\n",
    "                    random.randint(0, 255),\n",
    "                    random.randint(0, 255),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        for m_ann in modified_annotation:\n",
    "            modified_mask, state = polygon_to_mask(\n",
    "                modified_mask,\n",
    "                m_ann[\"points\"],\n",
    "                color=(\n",
    "                    random.randint(0, 255),\n",
    "                    random.randint(0, 255),\n",
    "                    random.randint(0, 255),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        Image.fromarray(sum_result_image.astype(\"uint8\")).convert(\"RGB\").save(\n",
    "            os.path.join(save_result_path, annotation[\"filename\"])\n",
    "        )\n",
    "        Image.fromarray(sum_draw_image.astype(\"uint8\")).convert(\"RGB\").save(\n",
    "            os.path.join(save_draw_path, annotation[\"filename\"])\n",
    "        )\n",
    "        Image.fromarray(sum_draw_refined_image.astype(\"uint8\")).convert(\"RGB\").save(\n",
    "            os.path.join(save_refined_draw_path, annotation[\"filename\"])\n",
    "        )\n",
    "        Image.fromarray(original_mask.astype(\"uint8\")).convert(\"RGB\").save(\n",
    "            os.path.join(save_annotation_draw_path, annotation[\"filename\"])\n",
    "        )\n",
    "        Image.fromarray(modified_mask.astype(\"uint8\")).convert(\"RGB\").save(\n",
    "            os.path.join(save_modified_annotation_draw_path, annotation[\"filename\"])\n",
    "        )\n",
    "        Image.fromarray(sum_mask_image.astype(\"uint8\")).convert(\"L\").save(\n",
    "            os.path.join(save_mask_path, annotation[\"filename\"])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 434\u001b[0m\n\u001b[1;32m    431\u001b[0m generated_spots \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iter_cnt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(generate_cnt):\n\u001b[0;32m--> 434\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mmake_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_indexs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 41\u001b[0m, in \u001b[0;36mmake_inputs\u001b[0;34m(annotation, target_indexs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     target_index \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(target_indexs)\n\u001b[0;32m---> 41\u001b[0m     rb_spot \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_coordinate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotation\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mannotations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# height, width 순\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rb_spot \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m can not generate right bottom spot\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(annotation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "Cell \u001b[0;32mIn[2], line 104\u001b[0m, in \u001b[0;36mrandom_coordinate\u001b[0;34m(annotation, target_index, height, width)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     mask, state \u001b[38;5;241m=\u001b[39m polygon_to_mask(mask, ann[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m], color\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 104\u001b[0m target_spots \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target_spots) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "from diffusers import StableDiffusionInpaintPipeline, ControlNetModel, DDIMScheduler\n",
    "from controlnet_aux.processor import MidasDetector\n",
    "import sys\n",
    "\n",
    "# Grounding DINO\n",
    "import groundingdino.datasets.transforms as T\n",
    "from groundingdino.models import build_model\n",
    "from groundingdino.util import box_ops\n",
    "from groundingdino.util.slconfig import SLConfig\n",
    "from groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "from groundingdino.util.inference import annotate, load_image, predict\n",
    "\n",
    "import supervision as sv\n",
    "\n",
    "# segment anything\n",
    "from segment_anything import build_sam, SamPredictor\n",
    "\n",
    "sys.path.insert(0, \"../harmonization\")\n",
    "from harmonization import Harmonization\n",
    "from gtgen.bpr import GtGenBPRInference\n",
    "\n",
    "\n",
    "def make_inputs(annotation, target_indexs):\n",
    "    height, width = annotation[\"metadata\"][\"height\"], annotation[\"metadata\"][\"width\"]\n",
    "    mask = np.zeros((height, width))\n",
    "    spot = None\n",
    "\n",
    "    while True:\n",
    "        target_index = random.choice(target_indexs)\n",
    "        rb_spot = random_coordinate(annotation[\"annotations\"], target_index, height, width)  # height, width 순\n",
    "\n",
    "        if rb_spot is None:\n",
    "            print(\"{} can not generate right bottom spot\".format(annotation[\"filename\"]))\n",
    "            return None\n",
    "\n",
    "        # rb_spot x값을 기준으로 height 선정 및 target_height 산출\n",
    "        target_height = random.randint(350, 450)\n",
    "        paste_mask = np.ones((target_height, target_height)) * 255\n",
    "\n",
    "        sum_mask = add_mask(mask, paste_mask, rb_spot[1], rb_spot[0])\n",
    "\n",
    "        if sum_mask is None:\n",
    "            continue\n",
    "\n",
    "        break\n",
    "\n",
    "    sum_mask = Image.fromarray(sum_mask.astype(\"uint8\")).convert(\"L\")\n",
    "\n",
    "    output = {\"mask\": sum_mask}\n",
    "    return output\n",
    "\n",
    "\n",
    "def make_grid(images, rows, cols):\n",
    "    w, h = images[0].size\n",
    "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
    "    for i, image in enumerate(images):\n",
    "        grid.paste(image, box=(i % cols * w, i // cols * h))\n",
    "    return grid\n",
    "\n",
    "\n",
    "def find_outer_contour_coordinates(mask):\n",
    "    # OpenCV의 findContours 함수를 사용하여 이진 이미지의 외곽선을 찾습니다.\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # 외곽선을 이루는 점들의 좌표를 반환합니다.\n",
    "    outer_contour_coords = [[], []]\n",
    "    for contour in contours:\n",
    "        for point in contour:\n",
    "            x, y = point[0]\n",
    "            outer_contour_coords[0].append(y)\n",
    "            outer_contour_coords[1].append(x)\n",
    "\n",
    "    return outer_contour_coords\n",
    "\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    return np.sqrt((point2[0] - point1[0]) ** 2 + (point2[1] - point1[1]) ** 2)\n",
    "\n",
    "\n",
    "def random_coordinate(annotation, target_index, height, width):\n",
    "    mask = np.zeros((height, width))\n",
    "    mask, state = polygon_to_mask(mask, annotation[target_index][\"points\"], color=255)\n",
    "\n",
    "    if not state:\n",
    "        return None\n",
    "\n",
    "    for idx, ann in enumerate(annotation):\n",
    "        if idx == target_index:\n",
    "            continue\n",
    "\n",
    "        mask, state = polygon_to_mask(mask, ann[\"points\"], color=0)\n",
    "\n",
    "    target_spots = np.argwhere(mask == 255).tolist()\n",
    "\n",
    "    if len(target_spots) == 0:\n",
    "        return None\n",
    "\n",
    "    coordinates = find_outer_contour_coordinates(mask)\n",
    "    threshold = 500\n",
    "\n",
    "    # 랜덤으로 좌표 선택\n",
    "    while True:\n",
    "        target_spot = random.choice(target_spots)  # height,width 순\n",
    "        distances = [\n",
    "            euclidean_distance((coord[0], coord[1]), target_spot) for coord in zip(coordinates[0], coordinates[1])\n",
    "        ]\n",
    "        min_distance = int(min(distances))\n",
    "\n",
    "        if min_distance >= threshold:\n",
    "            return target_spot\n",
    "        else:\n",
    "            threshold = threshold // 2\n",
    "\n",
    "\n",
    "def add_mask(mask, new_mask, right, bottom):\n",
    "    mask_cp = mask.copy()\n",
    "\n",
    "    # 새로운 마스크를 더할 위치 계산\n",
    "    left = right - new_mask.shape[1]\n",
    "    top = bottom - new_mask.shape[0]\n",
    "\n",
    "    # 마스크 영역에 새로운 마스크 더하기\n",
    "    if left < 0 or top < 0:\n",
    "        return None\n",
    "\n",
    "    mask_cp[top:bottom, left:right] += new_mask\n",
    "\n",
    "    return mask_cp\n",
    "\n",
    "\n",
    "def add_image(image, new_image, mask, right, bottom):\n",
    "    image_cp = image.copy()\n",
    "\n",
    "    # 새로운 마스크를 더할 위치 계산\n",
    "    left = right - new_image.shape[1]\n",
    "    top = bottom - new_image.shape[0]\n",
    "\n",
    "    # 마스크 영역에 새로운 마스크 더하기\n",
    "    if left < 0 or top < 0:\n",
    "        return None\n",
    "\n",
    "    for h in range(top, bottom):\n",
    "        for w in range(left, right):\n",
    "            if mask[h - top, w - left]:\n",
    "                image_cp[h, w, :] = new_image[h - top, w - left, :]\n",
    "\n",
    "    return image_cp\n",
    "\n",
    "\n",
    "def make_dirs(paths):\n",
    "    for path in paths:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def make_result(image, mask):\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    image_with_mask_contours = np.copy(image)\n",
    "    cv2.drawContours(image_with_mask_contours, contours, -1, (0, 0, 255), 2)\n",
    "    return image_with_mask_contours\n",
    "\n",
    "\n",
    "def mask_refinement(image, ann, bpr_inference):\n",
    "    seg_result = bpr_inference.inference(\n",
    "        img=image,\n",
    "        seg=ann,\n",
    "        img_scale=(256, 256),\n",
    "        img_ratios=[1.0, 2.0],\n",
    "        nms_iou_threshold=0.5,\n",
    "        point_density=0.25,\n",
    "        patch_size=[32, 64, 96],\n",
    "        padding=0,\n",
    "    )\n",
    "\n",
    "    height, width = image.shape[0], image.shape[1]\n",
    "    result_map = np.zeros((height, width))\n",
    "\n",
    "    for sr in seg_result[\"annotations\"]:\n",
    "        result_map, state = polygon_to_mask(result_map, sr[\"points\"], 255)\n",
    "\n",
    "    return result_map.astype(\"uint8\")\n",
    "\n",
    "\n",
    "def mask_to_polygon(mask):\n",
    "    # 윤곽선 찾기\n",
    "    contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # 윤곽선을 다각형으로 변환\n",
    "    polygons = []\n",
    "    for contour in contours:\n",
    "        contour = contour.squeeze(axis=1)  # 차원 축소\n",
    "        polygon = contour[:, [0, 1]].tolist()  # (y, x) 순서로 변환하여 리스트로 저장\n",
    "        polygons.append(polygon)\n",
    "\n",
    "    return polygons\n",
    "\n",
    "\n",
    "def polygon_to_mask(mask, polygons, color=255):\n",
    "    polygons = np.array(polygons, dtype=np.int32)\n",
    "    state = False\n",
    "\n",
    "    try:\n",
    "        mask = cv2.fillPoly(mask.astype(\"uint8\"), [polygons], color)\n",
    "        state = True\n",
    "    except:\n",
    "        print(\"mask passed!\")\n",
    "\n",
    "    return mask, state\n",
    "\n",
    "\n",
    "def modify_annotation(annotations, polygons, height, width):\n",
    "    # draw generated mask\n",
    "    generated_mask = np.zeros((height, width))\n",
    "    generated_annotations = []\n",
    "    original_annotations = []\n",
    "\n",
    "    for polygon in polygons:\n",
    "        generated_mask, state = polygon_to_mask(generated_mask, polygon, 255)\n",
    "\n",
    "        if state:\n",
    "            ann = {\n",
    "                \"id\": \"\",\n",
    "                \"type\": \"poly_seg\",\n",
    "                \"attributes\": {},\n",
    "                \"points\": polygon,\n",
    "                \"label\": \"rider\",\n",
    "            }\n",
    "            generated_annotations.append(ann)\n",
    "\n",
    "    for annotation in annotations:\n",
    "        # draw original mask\n",
    "        original_mask = np.zeros((height, width))\n",
    "        original_mask, state = polygon_to_mask(original_mask, annotation[\"points\"], 255)\n",
    "\n",
    "        if not state:\n",
    "            continue\n",
    "\n",
    "        # modify original mask\n",
    "        original_mask = np.where((original_mask == 255) & (generated_mask == 255), 0, original_mask)\n",
    "        original_polygons = mask_to_polygon(original_mask)\n",
    "\n",
    "        for polygon in original_polygons:\n",
    "            ann = copy.deepcopy(annotation)\n",
    "            ann[\"points\"] = polygon\n",
    "            original_annotations.append(ann)\n",
    "\n",
    "    original_annotations.extend(generated_annotations)\n",
    "    return original_annotations\n",
    "\n",
    "\n",
    "def load_model(model_config_path, model_checkpoint_path, device):\n",
    "    args = SLConfig.fromfile(model_config_path)\n",
    "    args.device = device\n",
    "    model = build_model(args)\n",
    "    checkpoint = torch.load(model_checkpoint_path, map_location=\"cpu\")\n",
    "    load_res = model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n",
    "    print(load_res)\n",
    "    _ = model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# detect object using grounding DINO\n",
    "def detect(image, image_source, text_prompt, model, box_threshold=0.5, text_threshold=0.35):\n",
    "    boxes, logits, phrases = predict(\n",
    "        model=model, image=image, caption=text_prompt, box_threshold=box_threshold, text_threshold=text_threshold\n",
    "    )\n",
    "\n",
    "    annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "    annotated_frame = annotated_frame[..., ::-1]  # BGR to RGB\n",
    "    return annotated_frame, boxes\n",
    "\n",
    "\n",
    "def segment(image, sam_model, boxes):\n",
    "    sam_model.set_image(image)\n",
    "    H, W, _ = image.shape\n",
    "    boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
    "\n",
    "    transformed_boxes = sam_model.transform.apply_boxes_torch(boxes_xyxy.to(device_2), image.shape[:2])\n",
    "    masks, _, _ = sam_model.predict_torch(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        boxes=transformed_boxes,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    return masks.cpu()\n",
    "\n",
    "\n",
    "def draw_mask(mask, image, random_color=True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.8])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "\n",
    "    annotated_frame_pil = Image.fromarray(image).convert(\"RGBA\")\n",
    "    mask_image_pil = Image.fromarray((mask_image.cpu().numpy() * 255).astype(np.uint8)).convert(\"RGBA\")\n",
    "\n",
    "    return np.array(Image.alpha_composite(annotated_frame_pil, mask_image_pil))\n",
    "\n",
    "\n",
    "def get_mask(image, dino, sam):\n",
    "    result_mask = np.zeros((image.shape[0], image.shape[1]))\n",
    "\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.RandomResize([800], max_size=1333),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    image_torch, _ = transform(Image.fromarray(image).convert(\"RGB\"), None)\n",
    "    annotated_frame, detected_boxes = detect(\n",
    "        image_torch, image, text_prompt=\"rider . bicycle . motorcycle .\", model=dino\n",
    "    )\n",
    "    if len(detected_boxes) == 0:\n",
    "        return None\n",
    "\n",
    "    seg_result = segment(image, sam, boxes=detected_boxes)\n",
    "    for seg_map in seg_result:\n",
    "        mask = seg_map[0].cpu().numpy().astype(np.uint8) * 255\n",
    "        result_mask = np.where(mask == 255, 255, result_mask)\n",
    "\n",
    "    return result_mask\n",
    "\n",
    "\n",
    "def crop_from_mask(image, mask):\n",
    "    # Find contours in the mask\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # If no contours found, return original image\n",
    "    if not contours:\n",
    "        return image\n",
    "\n",
    "    # Find the bounding box of the largest contour\n",
    "    x, y, w, h = cv2.boundingRect(contours[0])\n",
    "\n",
    "    # Crop the image using the bounding box\n",
    "    cropped_image = image[y : y + h, x : x + w]\n",
    "    cropped_mask = mask[y : y + h, x : x + w]\n",
    "\n",
    "    return cropped_image, cropped_mask\n",
    "\n",
    "\n",
    "device = \"cuda:2\"\n",
    "# torch.cuda.set_device(device)\n",
    "\n",
    "# 생성할 Annotation 정보\n",
    "base_image_path = \"/data/noah/dataset/magna_traffic_light/pre_images\"\n",
    "target_annotation_path = \"/data/noah/dataset/magna_traffic_light/pre_anno\"\n",
    "target_class_name = \"road\"\n",
    "target_height = None\n",
    "\n",
    "save_base_path = \"/data/noah/inference/magna_rv_inpainting\"\n",
    "save_result_path = os.path.join(save_base_path, \"results\")\n",
    "save_draw_path = os.path.join(save_base_path, \"draw_results\")\n",
    "save_refined_draw_path = os.path.join(save_base_path, \"draw_results_refined\")\n",
    "save_annotation_draw_path = os.path.join(save_base_path, \"annotation_draw\")\n",
    "save_modified_annotation_draw_path = os.path.join(save_base_path, \"modified_annotation_draw\")\n",
    "save_mask_path = os.path.join(save_base_path, \"masks\")\n",
    "make_dirs(\n",
    "    [\n",
    "        save_base_path,\n",
    "        save_result_path,\n",
    "        save_draw_path,\n",
    "        save_refined_draw_path,\n",
    "        save_modified_annotation_draw_path,\n",
    "        save_annotation_draw_path,\n",
    "        save_mask_path,\n",
    "    ]\n",
    ")\n",
    "\n",
    "device_2 = \"cuda:3\"\n",
    "harmonizer = Harmonization(\"/data/noah/ckpt/pretrain_ckpt/duconet/duconet1024.pth\", device=device_2)\n",
    "grounding_dino_ckpt_path = \"/data/noah/ckpt/pretrain_ckpt/Grounding_DINO/groundingdino_swinb_cogcoor.pth\"\n",
    "grounding_dino_config_path = (\n",
    "    \"/workspace/Grounded-Segment-Anything/GroundingDINO/groundingdino/config/GroundingDINO_SwinB.py\"\n",
    ")\n",
    "grounding_dino = load_model(grounding_dino_config_path, grounding_dino_ckpt_path, device=device_2)\n",
    "sam_ckpt_path = \"/data/noah/ckpt/pretrain_ckpt/SAM/sam_vit_h_4b8939.pth\"\n",
    "sam_predictor = SamPredictor(build_sam(checkpoint=sam_ckpt_path).to(device_2))\n",
    "\n",
    "pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    \"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/rv_inpaint_5.1\", torch_dtype=torch.float16\n",
    ").to(device)\n",
    "pipe.load_lora_weights(\n",
    "    \"/data/noah/ckpt/pretrain_ckpt/StableDiffusion/lora_detail\", weight_name=\"add_detail.safetensors\"\n",
    ")\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_freeu(s1=0.9, s2=0.2, b1=1.2, b2=1.4)\n",
    "# generator = torch.Generator(device=device).manual_seed(42)\n",
    "\n",
    "prompt = \"a bicycle rider is on the road, best quality, extremely detailed, clearness, naturalness, film grain, crystal clear, photo with color, actuality\"\n",
    "negative_prompt = \"cartoon, anime, painting, disfigured, immature, blur, picture, 3D, render, semi-realistic, drawing, poorly drawn, bad anatomy, wrong anatomy, gray scale, worst quality, low quality, sketch, text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck\"\n",
    "\n",
    "for ann_idx, ann_name in tqdm(enumerate(os.listdir(target_annotation_path)[:10])):\n",
    "    annotation_path = os.path.join(target_annotation_path, ann_name)\n",
    "\n",
    "    with open(annotation_path, \"r\") as f:\n",
    "        annotation = json.load(f)\n",
    "\n",
    "    target_indexs = []\n",
    "\n",
    "    for idx, ann in enumerate(annotation[\"annotations\"]):\n",
    "        if ann[\"label\"] == target_class_name:\n",
    "            target_indexs.append(idx)\n",
    "            break\n",
    "\n",
    "    if not len(target_indexs):\n",
    "        continue\n",
    "\n",
    "    height, width = annotation[\"metadata\"][\"height\"], annotation[\"metadata\"][\"width\"]\n",
    "    image = Image.open(os.path.join(base_image_path, annotation[\"parent_path\"][1:], annotation[\"filename\"]))\n",
    "\n",
    "    sum_mask_image = np.zeros((height, width))\n",
    "    sum_result_image = np.array(image).astype(\"uint8\")\n",
    "    sum_draw_image = None\n",
    "\n",
    "    generate_cnt = random.randint(3, 5)\n",
    "    inputs = None\n",
    "    generated_spots = []\n",
    "\n",
    "    for iter_cnt in range(generate_cnt):\n",
    "        inputs = make_inputs(annotation, target_indexs)\n",
    "\n",
    "        if inputs is None:\n",
    "            break\n",
    "\n",
    "        mask = np.array(inputs[\"mask\"]).astype(\"uint8\")\n",
    "        spot = np.argwhere(mask == 255)\n",
    "        right, bottom = np.max(spot[:, 1]), np.max(spot[:, 0])\n",
    "\n",
    "        if len(generated_spots) and np.any(np.all(np.isin(np.array(generated_spots), spot), axis=1)):\n",
    "            continue\n",
    "\n",
    "        result_image = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            image=image,\n",
    "            mask_image=inputs[\"mask\"],\n",
    "            height=image.height,\n",
    "            width=image.width,\n",
    "            num_inference_steps=25,\n",
    "            guidance_scale=9.5,\n",
    "            padding_mask_crop=32,\n",
    "            # generator=generator\n",
    "        ).images[0]\n",
    "\n",
    "        result_image = harmonizer.harmonize(np.array(result_image), mask)\n",
    "        result_image = result_image.astype(\"uint8\")\n",
    "\n",
    "        # cropped_img, _ = crop_from_mask(result_image, mask)\n",
    "        # cropped_mask = get_mask(cropped_img, grounding_dino, sam_predictor)\n",
    "        # if cropped_mask is None:\n",
    "        #     continue\n",
    "        # else:\n",
    "        #     cropped_mask = cropped_mask.astype(\"uint8\")\n",
    "        #     k = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n",
    "        #     cropped_mask = cv2.morphologyEx(cropped_mask, cv2.MORPH_OPEN, k, iterations=3)\n",
    "\n",
    "        # mask = add_mask(np.zeros((image.height, image.width)), cropped_mask, right, bottom)\n",
    "        # spot = np.argwhere(mask == 255).tolist()\n",
    "\n",
    "        generated_spots.extend(spot)\n",
    "\n",
    "        for st in spot:\n",
    "            sum_result_image[st[0], st[1], :] = result_image[st[0], st[1], :]\n",
    "\n",
    "        sum_mask_image = sum_mask_image + mask\n",
    "\n",
    "    if inputs is not None:\n",
    "        # mask refinement\n",
    "        polygons = mask_to_polygon(sum_mask_image)\n",
    "        generated_annotation = copy.deepcopy(annotation)\n",
    "        anns = []\n",
    "\n",
    "        for polygon in polygons:\n",
    "            an = {\n",
    "                \"id\": \"\",\n",
    "                \"type\": \"poly_seg\",\n",
    "                \"attributes\": {},\n",
    "                \"points\": polygon,\n",
    "                \"label\": \"rider\",\n",
    "            }\n",
    "            anns.append(an)\n",
    "\n",
    "        generated_annotation[\"annotations\"] = anns\n",
    "\n",
    "        # draw result image with mask\n",
    "        sum_draw_image = make_result(np.copy(sum_result_image), sum_mask_image.astype(\"uint8\"))\n",
    "\n",
    "        # annotation 수정 작업 #\n",
    "        modified_annotation = modify_annotation(\n",
    "            annotation[\"annotations\"],\n",
    "            polygons,\n",
    "            height,\n",
    "            width,\n",
    "        )\n",
    "\n",
    "        # draw annotation\n",
    "        original_mask = np.zeros((height, width, 3))\n",
    "        modified_mask = np.zeros((height, width, 3))\n",
    "\n",
    "        for ann in annotation[\"annotations\"]:\n",
    "            original_mask, state = polygon_to_mask(\n",
    "                original_mask,\n",
    "                ann[\"points\"],\n",
    "                color=(\n",
    "                    random.randint(0, 255),\n",
    "                    random.randint(0, 255),\n",
    "                    random.randint(0, 255),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        for m_ann in modified_annotation:\n",
    "            modified_mask, state = polygon_to_mask(\n",
    "                modified_mask,\n",
    "                m_ann[\"points\"],\n",
    "                color=(\n",
    "                    random.randint(0, 255),\n",
    "                    random.randint(0, 255),\n",
    "                    random.randint(0, 255),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        Image.fromarray(sum_result_image.astype(\"uint8\")).convert(\"RGB\").save(\n",
    "            os.path.join(save_result_path, annotation[\"filename\"])\n",
    "        )\n",
    "        Image.fromarray(sum_draw_image.astype(\"uint8\")).convert(\"RGB\").save(\n",
    "            os.path.join(save_draw_path, annotation[\"filename\"])\n",
    "        )\n",
    "        Image.fromarray(original_mask.astype(\"uint8\")).convert(\"RGB\").save(\n",
    "            os.path.join(save_annotation_draw_path, annotation[\"filename\"])\n",
    "        )\n",
    "        Image.fromarray(modified_mask.astype(\"uint8\")).convert(\"RGB\").save(\n",
    "            os.path.join(save_modified_annotation_draw_path, annotation[\"filename\"])\n",
    "        )\n",
    "        Image.fromarray(sum_mask_image.astype(\"uint8\")).convert(\"L\").save(\n",
    "            os.path.join(save_mask_path, annotation[\"filename\"])\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
