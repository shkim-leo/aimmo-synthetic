{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "\n",
    "dataset_folder = \"/home/sckim/Dataset/background/metadata.csv\"\n",
    "image_folder = os.path.join(dataset_folder, \"image\")\n",
    "text_folder = os.path.join(dataset_folder, \"text\")\n",
    "meta_path = os.path.join(dataset_folder, \"tag.csv\")\n",
    "\n",
    "text_names = sorted(os.listdir(text_folder))\n",
    "image_names = sorted(os.listdir(image_folder))\n",
    "\n",
    "data = []\n",
    "\n",
    "for text_file_name, image_file_name in zip(text_names, image_names):\n",
    "    text_file_path = os.path.join(text_folder, text_file_name)\n",
    "    image_file_path = os.path.join(image_folder, image_file_name)\n",
    "    caption = \"\"\n",
    "\n",
    "    with open(text_file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            caption = caption + line\n",
    "        f.close()\n",
    "\n",
    "    if \"no human\" in caption:\n",
    "        file_path = os.path.join(dataset_folder, image_file_name)\n",
    "        data.append({\"file_path\": file_path, \"text\": caption})\n",
    "        shutil.move(image_file_path, file_path)\n",
    "\n",
    "with open(meta_path, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"file_path\", \"text\"])\n",
    "\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in data:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"{meta_path} 파일이 생성되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from diffusers.blip.models.blip import blip_decoder\n",
    "\n",
    "\n",
    "def load_demo_image(image_path, image_size, device):\n",
    "    raw_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    w, h = raw_image.size\n",
    "    # display(raw_image.resize((w//5,h//5)))\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        ]\n",
    "    )\n",
    "    image = transform(raw_image).unsqueeze(0).to(device)\n",
    "    return image\n",
    "\n",
    "\n",
    "image_size = 512\n",
    "image_dir = \"/home/sckim/Dataset/background\"\n",
    "model_path = \"/home/sckim/Dataset/model_large_caption.pth\"\n",
    "device = \"cuda:0\"\n",
    "torch.cuda.set_device(torch.device(device))\n",
    "\n",
    "model = blip_decoder(pretrained=model_path, image_size=image_size, vit=\"large\")\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "file_name = \"/home/sckim/Dataset/background/metadata.csv\"\n",
    "data = []\n",
    "\n",
    "for img_name in tqdm(os.listdir(image_dir)):\n",
    "    if os.path.splitext(img_name)[-1] not in [\".jpg\", \".png\", \".jpeg\"]:\n",
    "        continue\n",
    "\n",
    "    image_path = os.path.join(image_dir, img_name)\n",
    "\n",
    "    image = load_demo_image(image_path=image_path, image_size=image_size, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # beam search\n",
    "        # caption = model.generate(image, sample=False, num_beams=3, max_length=40, min_length=5)\n",
    "        # nucleus sampling\n",
    "        caption = model.generate(image, sample=True, top_p=0.9, max_length=20, min_length=5)\n",
    "\n",
    "        data.append({\"file_path\": image_path, \"text\": caption[0]})\n",
    "\n",
    "with open(file_name, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"file_path\", \"text\"])\n",
    "\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in data:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"{file_name} 파일이 생성되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda:0\"\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "\n",
    "def read_csv(csv_path):\n",
    "    anno = []\n",
    "\n",
    "    with open(csv_path, \"r\", newline=\"\") as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        header = next(csv_reader)\n",
    "\n",
    "        for row in csv_reader:\n",
    "            anno.append(row)\n",
    "\n",
    "    return anno\n",
    "\n",
    "\n",
    "model = \"/home/sckim/Dataset/llama2_7b_chat_hf\"\n",
    "csv_path = \"/home/sckim/Dataset/background/tag.csv\"\n",
    "annotations = read_csv(csv_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "meta_list = [[], [], []]  # 0:weather, 1:place, 2:time\n",
    "pre_state = 0\n",
    "\n",
    "for ann in tqdm(annotations):\n",
    "    image_path, caption = ann[0], ann[1]\n",
    "\n",
    "    sequences = pipeline(\n",
    "        \"{}\\In the sentence above, tell me the words related to the weather, time, and place in each category.\".format(\n",
    "            caption\n",
    "        ),\n",
    "        do_sample=False,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=200,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "    print(sequences[0][\"generated_text\"])\n",
    "\n",
    "    # if \"*\" in sequences[0]['generated_text']:\n",
    "    #     for word in sequences[0]['generated_text'].split('\\n'):\n",
    "    #         if 'Weather' in word:\n",
    "    #             pre_state=0\n",
    "    #         elif 'Place' in word:\n",
    "    #             pre_state=1\n",
    "    #         elif 'Time' in word:\n",
    "    #             pre_state=2\n",
    "    #         elif '* ' in word:\n",
    "    #             text = word.split('* ')[-1]\n",
    "    #             meta_list[pre_state].append(text)\n",
    "    #         else:\n",
    "    #             continue\n",
    "    # else:\n",
    "    #     for word in sequences[0]['generated_text'].split('\\n'):\n",
    "    #         if 'Weather' in word:\n",
    "    #             pre_state=0\n",
    "    #             text = word.split(': ')[-1]\n",
    "    #             text = text.split(', ')\n",
    "    #             meta_list[pre_state].extend(text)\n",
    "    #         elif 'Place' in word:\n",
    "    #             pre_state=1\n",
    "    #             text = word.split(': ')[-1]\n",
    "    #             text = text.split(', ')\n",
    "    #             meta_list[pre_state].extend(text)\n",
    "    #         elif 'Time' in word:\n",
    "    #             pre_state=2\n",
    "    #             text = word.split(': ')[-1]\n",
    "    #             text = text.split(', ')\n",
    "    #             meta_list[pre_state].extend(text)\n",
    "    #         else:\n",
    "    #             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "meta_list = [list(set(meta)) for meta in meta_list]\n",
    "meta_weather_path = \"/home/sckim/Dataset/background/tag_weather.csv\"\n",
    "meta_place_path = \"/home/sckim/Dataset/background/tag_place.csv\"\n",
    "meta_time_path = \"/home/sckim/Dataset/background/tag_time.csv\"\n",
    "\n",
    "\n",
    "def write_csv(csv_path, data):\n",
    "    with open(csv_path, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"meta\"])\n",
    "\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in data:\n",
    "            writer.writerow({\"meta\": row})\n",
    "\n",
    "    print(f\"{csv_path} 파일이 생성되었습니다.\")\n",
    "\n",
    "\n",
    "write_csv(meta_weather_path, meta_list[0])\n",
    "write_csv(meta_place_path, meta_list[1])\n",
    "write_csv(meta_time_path, meta_list[2])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
