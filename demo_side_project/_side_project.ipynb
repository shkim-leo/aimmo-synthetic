{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, UNet2DConditionModel, LCMScheduler\n",
    "from diffusers.utils import make_image_grid, load_image\n",
    "from PIL import Image\n",
    "\n",
    "resolution = 512\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    \"/home/sckim/Dataset/lcm_sd_background/checkpoint-430000/unet\",\n",
    "    subfolder=\"unet\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"/home/sckim/Dataset/sd\", unet=unet, torch_dtype=torch.float16, safety_checker=None\n",
    ").to(\"cuda\")\n",
    "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "prompt = \"the old house with the stairs up to it is located next to some beautiful trees, anime, cartoon, masterpiece, natural\"\n",
    "\n",
    "generator = torch.manual_seed(11)\n",
    "background_image = pipe(\n",
    "    prompt,\n",
    "    num_inference_steps=4,\n",
    "    guidance_scale=7.0,\n",
    "    generator=generator,\n",
    "    height=resolution,\n",
    "    width=resolution,\n",
    ").images[0]\n",
    "display(background_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"GroundingDINO\"))\n",
    "\n",
    "import argparse\n",
    "import copy\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "from diffusers import StableDiffusionInpaintPipeline, LCMScheduler, UNet2DConditionModel\n",
    "from diffusers.utils import make_image_grid\n",
    "\n",
    "# Grounding DINO\n",
    "import groundingdino.datasets.transforms as T\n",
    "from groundingdino.models import build_model\n",
    "from groundingdino.util import box_ops\n",
    "from groundingdino.util.slconfig import SLConfig\n",
    "from groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "from groundingdino.util.inference import annotate, load_image, predict\n",
    "\n",
    "import supervision as sv\n",
    "\n",
    "# segment anything\n",
    "from segment_anything import build_sam, SamPredictor\n",
    "\n",
    "device = \"cuda:0\"\n",
    "torch.cuda.set_device(device)\n",
    "generator = torch.Generator(device=device).manual_seed(0)\n",
    "\n",
    "\n",
    "def load_model(model_config_path, model_checkpoint_path, device):\n",
    "    args = SLConfig.fromfile(model_config_path)\n",
    "    args.device = device\n",
    "    model = build_model(args)\n",
    "    checkpoint = torch.load(model_checkpoint_path, map_location=\"cpu\")\n",
    "    load_res = model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n",
    "    _ = model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def detect(image, image_tf, text_prompt, model, box_threshold=0.3, text_threshold=0.25):\n",
    "    boxes, logits, phrases = predict(\n",
    "        model=model, image=image_tf, caption=text_prompt, box_threshold=box_threshold, text_threshold=text_threshold\n",
    "    )\n",
    "\n",
    "    annotated_frame = annotate(image_source=image, boxes=boxes, logits=logits, phrases=phrases)\n",
    "    annotated_frame = annotated_frame[..., ::-1]  # BGR to RGB\n",
    "    return annotated_frame, boxes\n",
    "\n",
    "\n",
    "def segment(image, sam_model, boxes):\n",
    "    sam_model.set_image(image)\n",
    "    H, W, _ = image.shape\n",
    "    boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
    "\n",
    "    transformed_boxes = sam_model.transform.apply_boxes_torch(boxes_xyxy.to(device), image.shape[:2])\n",
    "    masks, _, _ = sam_model.predict_torch(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        boxes=transformed_boxes,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    return masks.cpu()\n",
    "\n",
    "\n",
    "def draw_mask(mask, image, random_color=True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.8])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "\n",
    "    annotated_frame_pil = Image.fromarray(image).convert(\"RGBA\")\n",
    "    mask_image_pil = Image.fromarray((mask_image.cpu().numpy() * 255).astype(np.uint8)).convert(\"RGBA\")\n",
    "\n",
    "    return np.array(Image.alpha_composite(annotated_frame_pil, mask_image_pil))\n",
    "\n",
    "\n",
    "grounding_dino_path = \"/home/sckim/Dataset/groundingdino_swinb_cogcoor.pth\"\n",
    "grounding_dino_config_filename = (\n",
    "    \"/workspace/Grounded-Segment-Anything/GroundingDINO/groundingdino/config/GroundingDINO_SwinB.py\"\n",
    ")\n",
    "grounding_dino_model = load_model(grounding_dino_config_filename, grounding_dino_path, device=device)\n",
    "\n",
    "sam_path = \"/home/sckim/Dataset/sam_vit_h_4b8939.pth\"\n",
    "sam_predictor = SamPredictor(build_sam(checkpoint=sam_path).to(device))\n",
    "\n",
    "image_path = \"/home/sckim/Dataset/background_inference/2.jpeg\"\n",
    "\n",
    "# load base and mask image\n",
    "image, image_tf = load_image(image_path)\n",
    "annotated_frame, detected_boxes = detect(image, image_tf, text_prompt=\"character .\", model=grounding_dino_model)\n",
    "\n",
    "\n",
    "segmented_frame_masks = segment(image, sam_predictor, boxes=detected_boxes)\n",
    "annotated_frame_with_mask = draw_mask(segmented_frame_masks[0][0], annotated_frame)\n",
    "\n",
    "mask = segmented_frame_masks[0][0].cpu().numpy()\n",
    "# inverted_mask = ((1 - mask) * 255).astype(np.uint8)\n",
    "\n",
    "image = Image.fromarray(image)\n",
    "mask = Image.fromarray(mask)\n",
    "\n",
    "image = image.resize((resolution, resolution))\n",
    "mask = mask.resize((resolution, resolution))\n",
    "\n",
    "########################################################################################################################\n",
    "pipeline = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    \"/home/sckim/Dataset/sd\",\n",
    "    torch_dtype=torch.float16,\n",
    "    # in_channels=9,\n",
    "    # low_cpu_mem_usage=False,\n",
    "    # ignore_mismatched_sizes=True,\n",
    "    safety_checker=None,\n",
    ")\n",
    "pipeline.load_ip_adapter(\n",
    "    \"/home/sckim/Dataset/ip_adapter/\",\n",
    "    subfolder=\"models\",\n",
    "    weight_name=\"ip-adapter_sd15.bin\",\n",
    ")\n",
    "pipeline = pipeline.to(device)\n",
    "\n",
    "prompt = \"remove\"\n",
    "negative_prmopt = \"\"\n",
    "\n",
    "result_image = pipeline(\n",
    "    prompt=prompt,\n",
    "    image=background_image,\n",
    "    mask_image=mask,\n",
    "    strength=1.0,\n",
    "    guidance_scale=9.0,\n",
    "    num_inference_steps=25,\n",
    "    height=resolution,\n",
    "    width=resolution,\n",
    "    ip_adapter_image=image,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "\n",
    "grid = make_image_grid(images=[image, mask, background_image, result_image], rows=1, cols=4)\n",
    "display(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"GroundingDINO\"))\n",
    "\n",
    "import argparse\n",
    "import copy\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "from diffusers import StableDiffusionInpaintPipeline, LCMScheduler, UNet2DConditionModel\n",
    "from diffusers.utils import make_image_grid\n",
    "\n",
    "# Grounding DINO\n",
    "import groundingdino.datasets.transforms as T\n",
    "from groundingdino.models import build_model\n",
    "from groundingdino.util import box_ops\n",
    "from groundingdino.util.slconfig import SLConfig\n",
    "from groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "from groundingdino.util.inference import annotate, load_image, predict\n",
    "\n",
    "import supervision as sv\n",
    "\n",
    "# segment anything\n",
    "from segment_anything import build_sam, SamPredictor\n",
    "\n",
    "device = \"cuda:0\"\n",
    "torch.cuda.set_device(device)\n",
    "generator = torch.Generator(device=device).manual_seed(42)\n",
    "\n",
    "\n",
    "def load_model(model_config_path, model_checkpoint_path, device):\n",
    "    args = SLConfig.fromfile(model_config_path)\n",
    "    args.device = device\n",
    "    model = build_model(args)\n",
    "    checkpoint = torch.load(model_checkpoint_path, map_location=\"cpu\")\n",
    "    load_res = model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n",
    "    _ = model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def detect(image, image_tf, text_prompt, model, box_threshold=0.3, text_threshold=0.25):\n",
    "    boxes, logits, phrases = predict(\n",
    "        model=model, image=image_tf, caption=text_prompt, box_threshold=box_threshold, text_threshold=text_threshold\n",
    "    )\n",
    "\n",
    "    annotated_frame = annotate(image_source=image, boxes=boxes, logits=logits, phrases=phrases)\n",
    "    annotated_frame = annotated_frame[..., ::-1]  # BGR to RGB\n",
    "    return annotated_frame, boxes\n",
    "\n",
    "\n",
    "def segment(image, sam_model, boxes):\n",
    "    sam_model.set_image(image)\n",
    "    H, W, _ = image.shape\n",
    "    boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
    "\n",
    "    transformed_boxes = sam_model.transform.apply_boxes_torch(boxes_xyxy.to(device), image.shape[:2])\n",
    "    masks, _, _ = sam_model.predict_torch(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        boxes=transformed_boxes,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    return masks.cpu()\n",
    "\n",
    "\n",
    "def draw_mask(mask, image, random_color=True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.8])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "\n",
    "    annotated_frame_pil = Image.fromarray(image).convert(\"RGBA\")\n",
    "    mask_image_pil = Image.fromarray((mask_image.cpu().numpy() * 255).astype(np.uint8)).convert(\"RGBA\")\n",
    "\n",
    "    return np.array(Image.alpha_composite(annotated_frame_pil, mask_image_pil))\n",
    "\n",
    "\n",
    "grounding_dino_path = \"/home/sckim/Dataset/groundingdino_swinb_cogcoor.pth\"\n",
    "grounding_dino_config_filename = (\n",
    "    \"/workspace/Grounded-Segment-Anything/GroundingDINO/groundingdino/config/GroundingDINO_SwinB.py\"\n",
    ")\n",
    "grounding_dino_model = load_model(grounding_dino_config_filename, grounding_dino_path, device=device)\n",
    "\n",
    "sam_path = \"/home/sckim/Dataset/sam_vit_h_4b8939.pth\"\n",
    "sam_predictor = SamPredictor(build_sam(checkpoint=sam_path).to(device))\n",
    "\n",
    "image_path = \"/home/sckim/Dataset/background_inference/2.jpeg\"\n",
    "\n",
    "# load base and mask image\n",
    "image, image_tf = load_image(image_path)\n",
    "annotated_frame, detected_boxes = detect(image, image_tf, text_prompt=\"character .\", model=grounding_dino_model)\n",
    "\n",
    "\n",
    "segmented_frame_masks = segment(image, sam_predictor, boxes=detected_boxes)\n",
    "annotated_frame_with_mask = draw_mask(segmented_frame_masks[0][0], annotated_frame)\n",
    "\n",
    "mask = segmented_frame_masks[0][0].cpu().numpy()\n",
    "inverted_mask = ((1 - mask) * 255).astype(np.uint8)\n",
    "\n",
    "image = Image.fromarray(image)\n",
    "mask = Image.fromarray(inverted_mask)\n",
    "\n",
    "############################################################################################################################\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    \"/home/sckim/Dataset/lcm_sd_background/checkpoint-430000/unet\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "pipeline = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    \"/home/sckim/Dataset/sd\",\n",
    "    unet=unet,\n",
    "    torch_dtype=torch.float16,\n",
    "    in_channels=9,\n",
    "    low_cpu_mem_usage=False,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    safety_checker=None,\n",
    ")\n",
    "pipeline.scheduler = LCMScheduler.from_config(pipeline.scheduler.config)\n",
    "# pipeline.enable_freeu(s1=0.9, s2=0.2, b1=1.2, b2=1.4)\n",
    "pipeline = pipeline.to(device)\n",
    "\n",
    "prompt = \"an anime scene with a clock tower and grassy field\"\n",
    "negative_prmopt = \"\"\n",
    "resolution = 512\n",
    "image = image.resize((resolution, resolution))\n",
    "mask = mask.resize((resolution, resolution))\n",
    "\n",
    "k = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "# mask = Image.fromarray(cv2.erode(np.array(mask), k))\n",
    "mask = Image.fromarray(cv2.dilate(np.array(mask), k))\n",
    "\n",
    "result_image = pipeline(\n",
    "    prompt=prompt,\n",
    "    image=image,\n",
    "    mask_image=mask,\n",
    "    strength=1.0,\n",
    "    guidance_scale=7.0,\n",
    "    num_inference_steps=5,\n",
    "    height=resolution,\n",
    "    width=resolution,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "\n",
    "grid = make_image_grid(images=[image, mask, result_image], rows=1, cols=3)\n",
    "display(grid)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
